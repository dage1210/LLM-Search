{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "''' \n",
    "@title ：基于大模型的搜索问答\n",
    "@description ：结合搜索引擎和大模型，从网络搜索相关内容，总结搜索结果，并给出相关问题。\n",
    "@author :Pantrick Zhang\n",
    "@date 2025-01-20 10:09:01\n",
    "@refence: https://github.com/shibing624/SearchGPT \n",
    "'''\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Dict\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import StreamingResponse, RedirectResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "import uuid\n",
    "\n",
    "class Language(Enum):\n",
    "    \"\"\"支持的语言枚举\"\"\"\n",
    "    EN = \"en\"\n",
    "    ZH = \"zh\"\n",
    "    \n",
    "class PromptManager:\n",
    "    \"\"\"提示词管理类\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._prompts = {\n",
    "            \"rag_system\": {\n",
    "                Language.EN: \"\"\"You are a large language AI assistant. You are given a user question, and please write clean, concise and accurate answer to the question. You will be given a set of related contexts to the question, each starting with a reference number like [[citation:x]], where x is a number. Please use the context and cite the context at the end of each sentence if applicable.\n",
    "\n",
    "Your answer must be correct, accurate and written by an expert using an unbiased and professional tone. Please keep your answer within 1024 tokens. If the provided context does not offer enough information, please use your own knowledge to answer the user question.\n",
    "\n",
    "Please cite the contexts with the reference numbers, in the format [citation:x]. If a sentence comes from multiple contexts, please list all applicable citations, like [citation:3][citation:5]. Other than code and specific names and citations, your answer must be written in the same language as the question.\n",
    "\"\"\",\n",
    "                Language.ZH: \"\"\"你是一个大型的语言AI助手。当用户提出问题时，请你写出清晰、简洁且准确的答案。我们会给你一组与问题相关的上下文，每个上下文都以类似[[citation:x]]这样的引用编号开始，其中x是一个数字。如果有引用[context]，请在每句话后面使用并引述该上下文。\n",
    "\n",
    "你的答案必须正确、精确，并由专家以公正和专业的语气撰写。请将你的回答限制在1024个token内。如果所提供的上下文信息不足，可以使用自己知识来回答用户问题。\n",
    "\n",
    "请按照[citation:x]格式引用带有参考编号的上下文。如果一句话来自多个上下文，请列出所有适用于此处引述，如[citation:3][citation:5]。除代码、特定名称和引述外，你必须使用与问题相同语言编写你的回答。\n",
    "\"\"\"\n",
    "            },\n",
    "            \"rag_qa\": {\n",
    "                Language.EN: \"\"\"[context]=```\n",
    "{context}\n",
    "```\n",
    "Current date: {current_date}\n",
    "\n",
    "Please answer the question with contexts, but don't blindly repeat the contexts verbatim. Please cite the contexts with the reference numbers, in the format [citation:x]. And here is the user question:\n",
    "\"\"\",\n",
    "                Language.ZH: \"\"\"[context]=```\n",
    "{context}\n",
    "```\n",
    "当前日期: {current_date}\n",
    "\n",
    "基于上下文回答问题，不要盲目地逐字重复上下文。请以[citation:x]的格式引用上下文。这是用户的问题：\n",
    "\"\"\"\n",
    "            },\n",
    "            \"related_system\": {\n",
    "                Language.EN: \"\"\"You assist users in posing relevant questions based on their original queries and related background. Please identify topics worth following up on, and write out questions that each do not exceed 10 tokens. You Can combine with historical messages.\"\"\",\n",
    "                Language.ZH: \"\"\"你帮助用户根据他们的原始问题和相关背景提出相关问题，可以结合历史消息。请确定值得跟进的主题，每个问题不超过10个token。\"\"\"\n",
    "            },\n",
    "            \"related_qa\": {\n",
    "                Language.EN: \"\"\"[context]=```\n",
    "{context}\n",
    "```\n",
    "\n",
    "based on the original question and related contexts, suggest three such further questions. Do NOT repeat the original question. Each related question should be no more than 10 tokens, separate 3 questions with `\\\\n`, do not add numbers before questions, just give question contents. Here is the original question:\n",
    "\"\"\",\n",
    "                Language.ZH: \"\"\"[context]=```\n",
    "{context}\n",
    "```\n",
    "\n",
    "根据原始问题和相关上下文，提出三个相似的问题。不要重复原始问题。每个相关问题应不超过10个token，问题前不要加序号，问题后加问号，`\\\\n`分隔多个问题。这是原始问题：\n",
    "\"\"\"\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def get_prompt(self, prompt_type: str, lang: Language, **kwargs) -> str:\n",
    "        \"\"\"获取提示词\n",
    "        \n",
    "        Args:\n",
    "            prompt_type: 提示词类型\n",
    "            lang: 语言\n",
    "            **kwargs: 格式化参数\n",
    "        \n",
    "        Returns:\n",
    "            格式化后的提示词\n",
    "        \"\"\"\n",
    "        prompt = self._prompts.get(prompt_type, {}).get(lang, \"\")\n",
    "        if not prompt:\n",
    "            raise ValueError(f\"Prompt not found for type {prompt_type} and language {lang}\")\n",
    "\n",
    "        if kwargs:\n",
    "            prompt = prompt.format(**kwargs)\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def detect_language(self, text: str) -> Language:\n",
    "        \"\"\"检测文本语言\n",
    "        \n",
    "        Args:\n",
    "            text: 待检测文本\n",
    "        \n",
    "        Returns:\n",
    "            检测到的语言\n",
    "        \"\"\"\n",
    "        return Language.ZH if any('\\u4e00' <= char <= '\\u9fff' for char in text) else Language.EN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-22 10:07:53.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_with_searchpro\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mSearching web for: 开源多模态大模型最新进展?\u001b[0m\n",
      "\u001b[32m2025-01-22 10:07:54.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msearch_with_searchpro\u001b[0m:\u001b[36m41\u001b[0m - \u001b[34m\u001b[1mSearch results: [{'name': '只有模仿没有创新？国产 AI 用行动打破质疑', 'url': 'https://finance.sina.com.cn/tech/roll/2025-01-17/doc-ineffyrh3466257.shtml', 'snippet': '摘要\\nMiniMax，选择用开源「震撼」一下全球同行。\\n作者  Li Yuan\\n编辑  郑玄\\n大模型时代已经正式迈入第三年。\\n回首过去的两年，不禁令人感慨。每年都有基座大模型架构已经尘埃落定的声音，然而每年，技术都在飞快地迭代创新，突破人们想象。\\n2024 年，OpenAI 的推理模型，通过对模型架构的创新，用 RL 的方法延续 Scaling Law，让大模型的智力水平持续进展；而中国公司也并没有落后，价格屠夫 DeepSeek 通过 MLA 的架构创新，让推理成本直接降低了一个数量级。\\n2025 年开年，令人欣喜的是，我们看到了一向在人们印象中是「低调做产品」的 MiniMax 公司，也加入了开源行列，将最先进的底层技术直接与社区和行业分享。\\n1 月 15 日，大模型公司 MiniMax 正式发布了 MiniMax-01 系列模型。它包括基础语言大模型 MiniMax-Text-01，和在其上集成了一个轻量级 ViT 模型而开发的视觉多模态大模型 MiniMax-VL-01。\\n「卷」起来的大模型公司，令人乐见。开源会提升创新效率，越来越好的基座模型之上，才搭建越来越有用的应用，进入千家万户，帮人们解放生产力。\\n这是 MiniMax 第一次发布开源模型，一出手就是一个炸裂模型架构创新：新模型采用了 MiniMax 独有的 Lightning Attention 机制，借鉴了 Linear Attention（线性注意力）机制，是全球第一次将 Linear Attention 机制引入到商业化规模的模型当中。\\n效果也是立竿见影，模型上下文长度直接达到了顶尖模型的 20-32 倍水平，推理时的上下文窗口能达到 400 万 token。模型效果立刻在海外上引起了关注。\\n模型的上下文窗口，指的是模型在生成每个新 token 时，实际参考的前面内容的范围。就像是模型能够一次性从书架上取下的书籍数量。模型的上下文窗口越大，模型生成时可以参考的信息量就越多，表现也就更加智能。\\n站在 2025 年年初的时间点，长上下文窗口还有一个新的意义：为模型的 Agent 能力，打下坚实基础。\\n业界公认，2025 年，Agent 能力将是 AI 届「卷生卷死」的重点，连 OpenAI 都在本周内推出了 Tasks，一个 AI Agent 的雏型。在 2025 年，我们很有可能看到越来越多真正「全自动的」AI，在我们的生活中起作用。甚至不同「'}, {'name': '只有模仿没有创新？国产 AI 用行动打破质疑', 'url': 'https://finance.jrj.com.cn/2025/01/17115247543522.shtml', 'snippet': 'MiniMax，选择用开源「震撼」一下全球同行。\\n大模型时代已经正式迈入第三年。\\n回首过去的两年，不禁令人感慨。每年都有基座大模型架构已经尘埃落定的声音，然而每年，技术都在飞快地迭代创新，突破人们想象。\\n2024 年，OpenAI 的推理模型，通过对模型架构的创新，用 RL 的方法延续 Scaling Law，让大模型的智力水平持续进展；而中国公司也并没有落后，价格屠夫 DeepSeek 通过 MLA 的架构创新，让推理成本直接降低了一个数量级。\\n2025 年开年，令人欣喜的是，我们看到了一向在人们印象中是「低调做产品」的 MiniMax 公司，也加入了开源行列，将最先进的底层技术直接与社区和行业分享。\\n1 月 15 日，大模型公司 MiniMax 正式发布了 MiniMax-01 系列模型。它包括基础语言大模型 MiniMax-Text-01，和在其上集成了一个轻量级 ViT 模型而开发的视觉多模态大模型 MiniMax-VL-01。\\n「卷」起来的大模型公司，令人乐见。开源会提升创新效率，越来越好的基座模型之上，才搭建越来越有用的应用，进入千家万户，帮人们解放生产力。\\n这是 MiniMax 第一次发布开源模型，一出手就是一个炸裂模型架构创新：新模型采用了 MiniMax 独有的 Lightening Attention 机制，借鉴了 Linear Attention（线性注意力）机制，是全球第一次将 Linear Attention 机制引入到商业化规模的模型当中。\\n效果也是立竿见影，模型上下文长度直接达到了顶尖模型的 20-32 倍水平，推理时的上下文窗口能达到 400 万 token。模型效果立刻在海外上引起了关注。\\n模型的上下文窗口，指的是模型在生成每个新 token 时，实际参考的前面内容的范围。就像是模型能够一次性从书架上取下的书籍数量。模型的上下文窗口越大，模型生成时可以参考的信息量就越多，表现也就更加智能。\\n站在 2025 年年初的时间点，长上下文窗口还有一个新的意义：为模型的 Agent 能力，打下坚实基础。\\n业界公认，2025 年，Agent 能力将是 AI 届「卷生卷死」的重点，连 OpenAI 都在本周内推出了 Tasks，一个 AI Agent 的雏型。在 2025 年，我们很有可能看到越来越多真正「全自动的」AI，在我们的生活中起作用。甚至不同「全自动的」的 AI 共同协作，帮我们处理事务。'}, {'name': 'MiniMax震撼开源，突破传统Transformer架构，4560亿参数，支持400万长上下文', 'url': 'https://www.jiqizhixin.com/articles/2025-01-15-9', 'snippet': '「2025 年，我们可能会看到第一批 AI Agent 加入劳动力大军，并对公司的生产力产生实质性的影响。」——OpenAI CEO Sam Altman\\n「2025 年，每个公司都将拥有 AI 软件工程师 Agent，它们会编写大量代码。」——Meta CEO Mark Zuckerberg\\n「未来，每家公司的 IT 部门都将成为 AI Agent 的 HR 部门。」—— 英伟达 CEO 黄仁勋\\n2025 新年伊始，在很多趋势都还不明朗的情况下，几位 AI 业界的重要人物几乎在同一时间做出了类似的判断 ——2025 年将是 AI Agent 之年。\\n没想到，MiniMax 很快就有了动作：开源了最新的基础语言模型 MiniMax-Text-01 和视觉多模态模型 MiniMax-VL-01。\\n新模型的最大亮点是，在业内首次大规模实现了新的线性注意力机制，这使得输入的上下文窗口大大变长：一次可处理 400 万 token，是其他模型的 20-32 倍。\\n他们相信，这些模型能够给接下来一年潜在 Agent 相关应用的爆发做出贡献。\\n为什么这项工作对于 Agent 如此重要？\\n随着 Agent 进入应用场景，无论是单个 Agent 工作时产生的记忆，还是多个 Agent 协作所产生的 context，都会对模型的长上下文窗口提出更多需求。\\n开源地址：https://github.com/MiniMax-AI\\nHugging Face：https://huggingface.co/MiniMaxAI\\n技术报告：https://filecdn.minimax.chat/ArxivMiniMax01Report.pdf\\nAPI：https://www.minimaxi.com/platform\\n一系列创新 造就比肩顶尖模型的开源模型 MiniMax-Text-01 究竟是如何炼成的？事实上，他们为此进行了一系列创新。从新型线性注意力到改进版混合专家架构，再到并行策略和通信技术的优化，MiniMax 解决了大模型在面对超长上下文时的多项效果与效率痛点。\\nLightning Attention 目前领先的 LLM 大都基于 Transformer，而 Transformer 核心的自注意力机制是其计算成本的重要来源。为了优化，研究社区可以说是绞尽脑汁，提出了稀疏注意力、低秩分解和线性注意力等许多技术。MiniMax 的 Lightning Attention 便是一种线性注意力。'}, {'name': '大模型专题：多模态大语言模型领域进展分享', 'url': 'https://www.sohu.com/a/839539744_121694397', 'snippet': '今天分享的是：大模型专题：多模态大语言模型领域进展分享\\n报告共计：42页\\n多模态大语言模型（MLLM）兴起，能处理多模态任务，如视觉问答、图表推理等。其架构包含编码器、连接器和大语言模型，数据训练分模态对齐和指令微调两阶段，评估方式多样。模型演进体现在分辨率提高、输入形式丰富、I/O模态支持增加，如支持多图视频输入、输出图文交错内容等。团队工作包括缓解幻觉，利用外部反馈和视觉基础模型提升准确性；构建长视频理解测评基准，发现开源模型有提升空间且字幕对理解有正向作用；提升多模态交互体验，如VITA无需唤醒词、支持打断输出和屏蔽噪声，其实现包括特殊状态token和双工机制。未来，MLLM需支持更长上下文、具备多模态Agent、实现轻量化部署、统一多模态生成与理解及训练和推理流程，以应对视觉token增多、长视频理解等需求，推动模型发展。\\n以下为报告节选内容\\n返回搜狐，查看更多\\n责任编辑：'}, {'name': '2025，见证更多GenAI应用落实！今年的中国AIGC产业峰会来啦', 'url': 'https://www.qbitai.com/2025/01/247128.html', 'snippet': '2025，见证更多GenAI应用落实！今年的中国AIGC产业峰会来啦\\n一水\\n量子位\\n仅一天时间，就能让你提前洞悉2025年AI发展趋势\\n过去一年，我们见证了AI无数个里程碑式”时刻。\\n从Sora带来的超写实视频生成能力，到开源大模型在性能上逐渐追平闭源模型；从多模态理解能力质的飞跃，到Agent技术在实际应用中的突破性进展\\n甚至就连AGI（通用人工智能）这种听起来遥不可及的词汇，也在大佬们的预测中逐渐接近。\\n无怪乎全网直呼，简直不敢想象2025年AI将如何发展？\\n此情此景下，一场由各行各业AI玩家代表总结过去、布局未来的大会，就这样应呼声而来。\\n仅一天时间，就能让你提前洞悉2025年AI发展趋势。\\n4月16日，我们将在北京举办第三届中国AIGC产业峰会，本届峰会主题为「万物皆可AI」。\\n届时我们将邀请更多AI应用层玩家，和AI基础设施、模型层的代表玩家，一同来分享对最新生成式AI现状与趋势的见解。\\n在往届的峰会上，我们邀请到了科技领域头部企业、AIGC垂直代表公司高管百度袁佛玉、微软大中华区关玮雅、美图CEO吴欣鸿、小冰COO徐元春等，科研领域的学者大咖智源研究院林咏华、人大高瓴卢志武教授等嘉宾分享交流。\\n峰会中有近千观众线下参会，300万观众观看线上直播，全网曝光2000万，获得了主流媒体的广泛关注与报道。\\n今年，我们诚邀你继续一同见证更多AI应用落地，并一探生成式AI的未来！\\n万物皆可AI的时代已经到来。\\n随着大模型浪潮深入，模型能力更强，应用成本更低，各行各业新产品新物种层出不穷。智能助手已经寻常，AI陪伴如火如荼，代码生成准备就绪，视频生成越来越强，生产力和互动娱乐交相辉映，软件硬件一浪高过一浪。\\n还有不用AI的吗？还有没让AI赋能的吗？万物还能怎样AI起来？\\n2025年AIGC产业峰会，我们以「万物皆可AI」为主题，聚焦「万物皆可AI」，诚邀AI创业者、开发者、资深使用者，让更多的AI落地被看见，让更多人用上AI、用好AI，被AI加速成长。\\n欢迎来到，「万物皆可AI」时代。\\n过去一年里AIGC产业风起云涌，无数优秀的企业、产品涌现而出，量子位将根据过去一年里AIGC企业、产品的表现与反馈，结合对2024年技术与场景的观察，评选出2025年值得关注的AIGC企业、 2025年值得关注的AIGC产品两类奖项。\\n产品创新力关注产品在功能、体验和应用场景方面的创新性和独特性；\\n本次评选即日起开始报名，截至2025年3月24日，'}, {'name': '通信行业点评报告：字节发布豆包实时语音模型，重视AI产业链', 'url': 'https://stock.stockstar.com/JC2025012100005011.shtml', 'snippet': '（以下内容从开源证券《通信行业点评报告：字节发布豆包实时语音模型，重视AI产业链》研报附件原文摘录）字节发布豆包实时语音大模型发布，AI版图持续扩张2025年1月20日，字节跳动发布豆包实时语音大模型，该模型是一款语音理解和生成一体化的模型，实现了端到端语音对话。相比传统级联模式，该模型在语音表现力、控制力、情绪承接方面表现较强，并具备低时延、对话中可随时打断等特性。在数据收集中，团队筛选并整理大量包含丰富情感的语音数据，涵盖各种场景与情绪状态。在预训练阶段，团队对各模态交织数据进行深入训练，精准捕捉并高效压缩海量语音信息，通过Scaling，最大程度实现语音与文本能力深度融合和能力涌现。在后训练阶段，团队使用了高质量数据与RL算法，进一步提供模型高情商对话能力与安全性，并使大模型同时兼具智商”与情商”。架构支持多模态输入输出，涌现出较强声音控制、声音扮演等能力在架构方面，豆包团队研发端到端框架，深度融合语音与文本模态，该框架面向语音生成和理解进行统一联合建模，实现多模态输入和输出效果，涵盖S2S（语音到语音）、S2T（语音到文本）、T2S（文本到语音）、T2T（文本到文本）等多种模式。模型涌现多种能力：在声音控制方面，模型不仅能依照基础指令输出，还可遵循丰富的复杂指令；在声音扮演方面，目前模型部分方言和口音，主要源自于预训练阶段数据泛化，而非针对性训练。此外，模型支持实时联网功能，能够根据问题，动态获取最新信息，问题回答时效性强。从测评结来看，豆包实时语音大模型在情绪理解和情感表达方面优势明显，整体满意度方面，豆包实时语音大模型评分为4.36，GPT-4o为3.18，豆包模型表现更优。ToB和ToC端大模型齐飞，重视字节跳动等相关产业链投资机会ToB和ToC端大模型齐飞，建议重视国产算力产业链：（1）【国产算力芯片】推荐标的：中兴通讯；受益标的：寒武纪、海光信息等；（2）【字节/阿里/腾讯/百度等AIDC供应商】推荐标的：润泽科技、宝信软件；受益标的：光环新网、世纪互联、大位科技、东方国信、云赛智联、奥飞数据、万国数据、科华数据等；（3）【液冷】推荐标的：英维克；受益标的：申菱环境、同飞股份、网宿科技、科华数据、高澜股份、申菱环境、依米康、飞荣达等；（4）【服务器电源】受益标的：欧陆通、麦格米特等；（5）【柴油发电机】受益标的：科泰电源、潍柴重机等；（6）【变压器】受益标的：金盘科技等；（7）【铜连接】受益标的：'}, {'name': '中文大模型基准测评2024年10月报告-2024年度中文大模型阶段性进展评估', 'url': 'https://www.sohu.com/a/826962523_121825883', 'snippet': '2024年10月的SuperCLUE报告聚焦于中文大模型的最新进展。报告指出，OpenAI的o1-preview在全球大模型竞争中领先，国内模型如Qwen2.5-72B-Instruct和DeepSeek V2.5在全球开源模型中表现突出。\\n国内闭源模型GLM-4-Plus、SenseChat 5.5、AndesGPT-2.0与国际模型差距缩小。报告还强调了端侧小模型的快速发展，特别是在中文场景下的应用潜力。\\n报告目录：\\n报告核心结论摘要\\n国内大模型关键进展及趋势\\nSuperCLUE通用能力测评\\nSuperCLUE专项与行业基准测评\\nSuperCLUE多模态能力测评\\n精准量化通用人工智能（AGI）进展，定义人类迈向AGI的路线图\\n报告部分内容节选如下：\\n返回搜狐，查看更多\\n责任编辑：'}, {'name': '首个可保留情感的音频LLM！Meta重磅开源7B-Spirit LM，一网打尽「音频+文本」多模态任务', 'url': 'https://www.163.com/dy/article/JHJO9R3I0511ABV6.html', 'snippet': '新智元报道\\n编辑：LRS\\n【新智元导读】Meta最近开源了一个7B尺寸的Spirit LM的多模态语言模型，能够理解和生成语音及文本，可以非常自然地在两种模式间转换，不仅能处理基本的语音转文本和文本转语音任务，还能捕捉和再现语音中的情感和风格。\\n在纯文本大模型取得进展的同时，其他模态数据，如语音与文本结合的语言模型（SpeechLMs）也成为了一个热门的研究领域，但现有的模型要么在仅包含语音的数据上进行训练，要么是关注特定任务，如文本转语音（TTS）、自动语音识别（ASR）或翻译，在其他模态数据和任务上的泛化能力十分有限。\\n在大型语言模型（LLM）性能不断提升的情况下，一个常用的方法是先用ASR模型将语音转录成文本，然后用文本模型来生成新的文本，最后再用TTS模型将文本转换成语音，这种流程的一个显著缺陷就是语音表达性不佳，语言模型无法建模并生成富有表现力的语音数据。\\n最近，Meta开源了一个基础多模态语言模型Spirit LM，基于一个70亿参数的预训练文本语言模型，交错使用文本和语音数据进行训练，使模型能够自由地混合文本和语音，在任一模态中生成语言内容。\\n项目主页：https://speechbot.github.io/spiritlm/\\n论文链接：https://arxiv.org/pdf/2402.05755\\n代码链接：https://github.com/facebookresearch/spiritlm\\n开源链接：https://huggingface.co/spirit-lm/Meta-spirit-lm\\n将语音和文本序列拼接成一条token流，并使用一个小型的、自动整理（automatically-curated）的语音-文本平行语料库，采用逐词交错的方法进行训练。\\nSpirit LM有两个版本：基础版（Base）使用语音音素单元（HuBERT），表达版（Expressive）还额外使用音高和风格单元来模拟表达性，以增强模型在生成语音时的表现力，也就是说模型不仅能够理解和生成基本的语音和文本，还能在表达情感和风格方面表现得更加丰富和自然。\\n对于两个版本的模型，文本都使用子词BPE标记进行编码，最终得到的模型既展现了文本模型的语义能力，也具备语音模型的表达能力；模型还能够在少量样本的情况下跨模态学习新任务（例如自动语音识别、文本转语音、语音分类）。\\n不过需要注意的是，和其他预训练模型一样，Sprit LM也可能会生成一些不安全的内容，'}, {'name': '腾讯混元上线文生视频并开源，120秒内成片！还有提示词建议', 'url': 'https://www.sohu.com/a/832903527_115978', 'snippet': '智东西\\n作者  汪越\\n编辑  漠影\\n智东西12月3日报道，今天，腾讯混元大模型正式上线视频生成能力，这是在腾讯文生文、文生图、3D生成之后的最新技术进展。\\n据腾讯混元多模态生成技术负责人凯撒现场介绍，此次更新中，HunYuan-Video模型经历了四项核心改进：\\n1、引入超大规模数据处理系统，提升视频画质；\\n2、采用多模态大语言模型（MLLM），优化文本与图像的对齐；\\n3、使用130亿参数的全注意力机制（DIT）和双模态ScalingLaw，增强时空建模与动态表现；\\n4、采用自研3D VAE架构，提升图像和视频的重建能力。\\n与此同时，腾讯宣布将这款拥有130亿参数规模的视频生成模型开源。目前，该模型已在APP与Web端发布，其标准模式下的视频生成大约需要120秒完成。\\n一、腾讯HunYuan-Video模型技术升级与应用拓展\\n腾讯对HunYuan-Video模型进行了四项技术升级，涵盖了数据处理系统、文本编码、算力优化等多个方面，提升了视频生成的质量与可控性。此外，腾讯还通过微调、应用拓展及开源等措施进一步强化了模型的实际应用能力。\\n1、四项关键技术升级\\n首先，模型采用了一个超大规模的数据处理系统，能够混合处理图像与视频数据。该系统包括文字检测、转景检测、美学打分、动作检测、准确度检测等多个维度的功能，进一步提升视频画质。\\n其次，模型引入了多模态大语言模型（Decoder-only MLLM）作为文本编码器，提升了复杂文本的理解能力，同时支持多语言理解。这一升级使得文本与图像之间的对齐性得到了加强，能够根据用户提供的提示词精确生成符合要求的视频内容。\\n另外，模型架构使用了130亿参数的全注意力机制（DIT）和双模态ScalingLaw，能够在视频生成中有效利用算力和数据资源，增强时空建模能力，并优化视频生成过程中的动态表现。此架构支持原生转场，可实现了多个镜头间的自然切换，并保持主体一致性。\\n最后，HunYuan-Video采用了自研的3D VAE架构，以提升图像和视频重建的能力，特别在小人脸和大幅运动场景下表现更加流畅。\\n2、六大微调领域强化定向能力\\n在预训练之后，腾讯混元大模型目前正在进行微调（SFT）工作，进一步增强其视频生成的定向能力。HunYuan-Video在六个关键方面进行了专项微调，包括画质优化、高动态效果、艺术镜头、手写文本、转场效果以及连续动作的生成，其中一些调整仍在进行中。\\n3、Recaption模型与两种生成模式'}, {'name': '智源研究院与腾讯达成战略合作，推动大模型技术前沿探索和应用落地', 'url': 'https://blog.csdn.net/eagleofstar/article/details/144589920', 'snippet': '2024 年 12 月 18日， 智源研究院与腾讯签署战略合作协议，双方将在大模型研发、人工智能技术前沿探索及开源生态建设等领域展开深入合作。智源研究院院长王仲远、副院长兼总工程师林咏华，腾讯集团高级执行副总裁、云与智慧产业事业群总裁汤道生、腾讯云副总裁、腾讯教育负责人石梅等出席签约仪式。\\n此次战略合作，双方将充分发挥各自优势，共同推动大模型技术与产业场景深度融合，探索多元算力下的大模型训练与推理优化技术解决方案，构建开放创新的软硬件生态体系等。\\n智源研究院院长王仲远在签约仪式上表示，智源研究院是专注于人工智能技术原始创新的新型研发机构，持续引领大模型的技术发展方向，布局多模态世界模型、具身智能、AI for Science 等前沿领域并取得了一系列的国际领先的科研成果与进展；同时，推出了包括模型、数据、算法、评测、系统的大模型全栈开源技术基座以及面向大模型、支持多种异构算力的智算集群软件栈，以赋能产业应用与技术生态的发展。此次，智源研究院携手腾讯，将促进产研协同发展，推动技术转化与产业落地，形成从实验室研究到赋能千行百业的技术生态”。\\n腾讯集团高级执行副总裁，云与智慧产业事业群CEO 汤道生表示，腾讯致力于打造离产业最近的AI，将AI融入到产业场景中。希望腾讯充分发挥在云计算、大数据、人工智能等领域的技术积累，与智源人工智能研究院的研究成果相互融合，共同探索大模型技术的新应用、新场景，探索人工智能技术的无限潜力。'}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'name': '只有模仿没有创新？国产 AI 用行动打破质疑', 'url': 'https://finance.sina.com.cn/tech/roll/2025-01-17/doc-ineffyrh3466257.shtml', 'snippet': '摘要\\nMiniMax，选择用开源「震撼」一下全球同行。\\n作者  Li Yuan\\n编辑  郑玄\\n大模型时代已经正式迈入第三年。\\n回首过去的两年，不禁令人感慨。每年都有基座大模型架构已经尘埃落定的声音，然而每年，技术都在飞快地迭代创新，突破人们想象。\\n2024 年，OpenAI 的推理模型，通过对模型架构的创新，用 RL 的方法延续 Scaling Law，让大模型的智力水平持续进展；而中国公司也并没有落后，价格屠夫 DeepSeek 通过 MLA 的架构创新，让推理成本直接降低了一个数量级。\\n2025 年开年，令人欣喜的是，我们看到了一向在人们印象中是「低调做产品」的 MiniMax 公司，也加入了开源行列，将最先进的底层技术直接与社区和行业分享。\\n1 月 15 日，大模型公司 MiniMax 正式发布了 MiniMax-01 系列模型。它包括基础语言大模型 MiniMax-Text-01，和在其上集成了一个轻量级 ViT 模型而开发的视觉多模态大模型 MiniMax-VL-01。\\n「卷」起来的大模型公司，令人乐见。开源会提升创新效率，越来越好的基座模型之上，才搭建越来越有用的应用，进入千家万户，帮人们解放生产力。\\n这是 MiniMax 第一次发布开源模型，一出手就是一个炸裂模型架构创新：新模型采用了 MiniMax 独有的 Lightning Attention 机制，借鉴了 Linear Attention（线性注意力）机制，是全球第一次将 Linear Attention 机制引入到商业化规模的模型当中。\\n效果也是立竿见影，模型上下文长度直接达到了顶尖模型的 20-32 倍水平，推理时的上下文窗口能达到 400 万 token。模型效果立刻在海外上引起了关注。\\n模型的上下文窗口，指的是模型在生成每个新 token 时，实际参考的前面内容的范围。就像是模型能够一次性从书架上取下的书籍数量。模型的上下文窗口越大，模型生成时可以参考的信息量就越多，表现也就更加智能。\\n站在 2025 年年初的时间点，长上下文窗口还有一个新的意义：为模型的 Agent 能力，打下坚实基础。\\n业界公认，2025 年，Agent 能力将是 AI 届「卷生卷死」的重点，连 OpenAI 都在本周内推出了 Tasks，一个 AI Agent 的雏型。在 2025 年，我们很有可能看到越来越多真正「全自动的」AI，在我们的生活中起作用。甚至不同「'}\n",
      "{'name': '只有模仿没有创新？国产 AI 用行动打破质疑', 'url': 'https://finance.jrj.com.cn/2025/01/17115247543522.shtml', 'snippet': 'MiniMax，选择用开源「震撼」一下全球同行。\\n大模型时代已经正式迈入第三年。\\n回首过去的两年，不禁令人感慨。每年都有基座大模型架构已经尘埃落定的声音，然而每年，技术都在飞快地迭代创新，突破人们想象。\\n2024 年，OpenAI 的推理模型，通过对模型架构的创新，用 RL 的方法延续 Scaling Law，让大模型的智力水平持续进展；而中国公司也并没有落后，价格屠夫 DeepSeek 通过 MLA 的架构创新，让推理成本直接降低了一个数量级。\\n2025 年开年，令人欣喜的是，我们看到了一向在人们印象中是「低调做产品」的 MiniMax 公司，也加入了开源行列，将最先进的底层技术直接与社区和行业分享。\\n1 月 15 日，大模型公司 MiniMax 正式发布了 MiniMax-01 系列模型。它包括基础语言大模型 MiniMax-Text-01，和在其上集成了一个轻量级 ViT 模型而开发的视觉多模态大模型 MiniMax-VL-01。\\n「卷」起来的大模型公司，令人乐见。开源会提升创新效率，越来越好的基座模型之上，才搭建越来越有用的应用，进入千家万户，帮人们解放生产力。\\n这是 MiniMax 第一次发布开源模型，一出手就是一个炸裂模型架构创新：新模型采用了 MiniMax 独有的 Lightening Attention 机制，借鉴了 Linear Attention（线性注意力）机制，是全球第一次将 Linear Attention 机制引入到商业化规模的模型当中。\\n效果也是立竿见影，模型上下文长度直接达到了顶尖模型的 20-32 倍水平，推理时的上下文窗口能达到 400 万 token。模型效果立刻在海外上引起了关注。\\n模型的上下文窗口，指的是模型在生成每个新 token 时，实际参考的前面内容的范围。就像是模型能够一次性从书架上取下的书籍数量。模型的上下文窗口越大，模型生成时可以参考的信息量就越多，表现也就更加智能。\\n站在 2025 年年初的时间点，长上下文窗口还有一个新的意义：为模型的 Agent 能力，打下坚实基础。\\n业界公认，2025 年，Agent 能力将是 AI 届「卷生卷死」的重点，连 OpenAI 都在本周内推出了 Tasks，一个 AI Agent 的雏型。在 2025 年，我们很有可能看到越来越多真正「全自动的」AI，在我们的生活中起作用。甚至不同「全自动的」的 AI 共同协作，帮我们处理事务。'}\n",
      "{'name': 'MiniMax震撼开源，突破传统Transformer架构，4560亿参数，支持400万长上下文', 'url': 'https://www.jiqizhixin.com/articles/2025-01-15-9', 'snippet': '「2025 年，我们可能会看到第一批 AI Agent 加入劳动力大军，并对公司的生产力产生实质性的影响。」——OpenAI CEO Sam Altman\\n「2025 年，每个公司都将拥有 AI 软件工程师 Agent，它们会编写大量代码。」——Meta CEO Mark Zuckerberg\\n「未来，每家公司的 IT 部门都将成为 AI Agent 的 HR 部门。」—— 英伟达 CEO 黄仁勋\\n2025 新年伊始，在很多趋势都还不明朗的情况下，几位 AI 业界的重要人物几乎在同一时间做出了类似的判断 ——2025 年将是 AI Agent 之年。\\n没想到，MiniMax 很快就有了动作：开源了最新的基础语言模型 MiniMax-Text-01 和视觉多模态模型 MiniMax-VL-01。\\n新模型的最大亮点是，在业内首次大规模实现了新的线性注意力机制，这使得输入的上下文窗口大大变长：一次可处理 400 万 token，是其他模型的 20-32 倍。\\n他们相信，这些模型能够给接下来一年潜在 Agent 相关应用的爆发做出贡献。\\n为什么这项工作对于 Agent 如此重要？\\n随着 Agent 进入应用场景，无论是单个 Agent 工作时产生的记忆，还是多个 Agent 协作所产生的 context，都会对模型的长上下文窗口提出更多需求。\\n开源地址：https://github.com/MiniMax-AI\\nHugging Face：https://huggingface.co/MiniMaxAI\\n技术报告：https://filecdn.minimax.chat/ArxivMiniMax01Report.pdf\\nAPI：https://www.minimaxi.com/platform\\n一系列创新 造就比肩顶尖模型的开源模型 MiniMax-Text-01 究竟是如何炼成的？事实上，他们为此进行了一系列创新。从新型线性注意力到改进版混合专家架构，再到并行策略和通信技术的优化，MiniMax 解决了大模型在面对超长上下文时的多项效果与效率痛点。\\nLightning Attention 目前领先的 LLM 大都基于 Transformer，而 Transformer 核心的自注意力机制是其计算成本的重要来源。为了优化，研究社区可以说是绞尽脑汁，提出了稀疏注意力、低秩分解和线性注意力等许多技术。MiniMax 的 Lightning Attention 便是一种线性注意力。'}\n",
      "{'name': '大模型专题：多模态大语言模型领域进展分享', 'url': 'https://www.sohu.com/a/839539744_121694397', 'snippet': '今天分享的是：大模型专题：多模态大语言模型领域进展分享\\n报告共计：42页\\n多模态大语言模型（MLLM）兴起，能处理多模态任务，如视觉问答、图表推理等。其架构包含编码器、连接器和大语言模型，数据训练分模态对齐和指令微调两阶段，评估方式多样。模型演进体现在分辨率提高、输入形式丰富、I/O模态支持增加，如支持多图视频输入、输出图文交错内容等。团队工作包括缓解幻觉，利用外部反馈和视觉基础模型提升准确性；构建长视频理解测评基准，发现开源模型有提升空间且字幕对理解有正向作用；提升多模态交互体验，如VITA无需唤醒词、支持打断输出和屏蔽噪声，其实现包括特殊状态token和双工机制。未来，MLLM需支持更长上下文、具备多模态Agent、实现轻量化部署、统一多模态生成与理解及训练和推理流程，以应对视觉token增多、长视频理解等需求，推动模型发展。\\n以下为报告节选内容\\n返回搜狐，查看更多\\n责任编辑：'}\n",
      "{'name': '2025，见证更多GenAI应用落实！今年的中国AIGC产业峰会来啦', 'url': 'https://www.qbitai.com/2025/01/247128.html', 'snippet': '2025，见证更多GenAI应用落实！今年的中国AIGC产业峰会来啦\\n一水\\n量子位\\n仅一天时间，就能让你提前洞悉2025年AI发展趋势\\n过去一年，我们见证了AI无数个里程碑式”时刻。\\n从Sora带来的超写实视频生成能力，到开源大模型在性能上逐渐追平闭源模型；从多模态理解能力质的飞跃，到Agent技术在实际应用中的突破性进展\\n甚至就连AGI（通用人工智能）这种听起来遥不可及的词汇，也在大佬们的预测中逐渐接近。\\n无怪乎全网直呼，简直不敢想象2025年AI将如何发展？\\n此情此景下，一场由各行各业AI玩家代表总结过去、布局未来的大会，就这样应呼声而来。\\n仅一天时间，就能让你提前洞悉2025年AI发展趋势。\\n4月16日，我们将在北京举办第三届中国AIGC产业峰会，本届峰会主题为「万物皆可AI」。\\n届时我们将邀请更多AI应用层玩家，和AI基础设施、模型层的代表玩家，一同来分享对最新生成式AI现状与趋势的见解。\\n在往届的峰会上，我们邀请到了科技领域头部企业、AIGC垂直代表公司高管百度袁佛玉、微软大中华区关玮雅、美图CEO吴欣鸿、小冰COO徐元春等，科研领域的学者大咖智源研究院林咏华、人大高瓴卢志武教授等嘉宾分享交流。\\n峰会中有近千观众线下参会，300万观众观看线上直播，全网曝光2000万，获得了主流媒体的广泛关注与报道。\\n今年，我们诚邀你继续一同见证更多AI应用落地，并一探生成式AI的未来！\\n万物皆可AI的时代已经到来。\\n随着大模型浪潮深入，模型能力更强，应用成本更低，各行各业新产品新物种层出不穷。智能助手已经寻常，AI陪伴如火如荼，代码生成准备就绪，视频生成越来越强，生产力和互动娱乐交相辉映，软件硬件一浪高过一浪。\\n还有不用AI的吗？还有没让AI赋能的吗？万物还能怎样AI起来？\\n2025年AIGC产业峰会，我们以「万物皆可AI」为主题，聚焦「万物皆可AI」，诚邀AI创业者、开发者、资深使用者，让更多的AI落地被看见，让更多人用上AI、用好AI，被AI加速成长。\\n欢迎来到，「万物皆可AI」时代。\\n过去一年里AIGC产业风起云涌，无数优秀的企业、产品涌现而出，量子位将根据过去一年里AIGC企业、产品的表现与反馈，结合对2024年技术与场景的观察，评选出2025年值得关注的AIGC企业、 2025年值得关注的AIGC产品两类奖项。\\n产品创新力关注产品在功能、体验和应用场景方面的创新性和独特性；\\n本次评选即日起开始报名，截至2025年3月24日，'}\n",
      "{'name': '通信行业点评报告：字节发布豆包实时语音模型，重视AI产业链', 'url': 'https://stock.stockstar.com/JC2025012100005011.shtml', 'snippet': '（以下内容从开源证券《通信行业点评报告：字节发布豆包实时语音模型，重视AI产业链》研报附件原文摘录）字节发布豆包实时语音大模型发布，AI版图持续扩张2025年1月20日，字节跳动发布豆包实时语音大模型，该模型是一款语音理解和生成一体化的模型，实现了端到端语音对话。相比传统级联模式，该模型在语音表现力、控制力、情绪承接方面表现较强，并具备低时延、对话中可随时打断等特性。在数据收集中，团队筛选并整理大量包含丰富情感的语音数据，涵盖各种场景与情绪状态。在预训练阶段，团队对各模态交织数据进行深入训练，精准捕捉并高效压缩海量语音信息，通过Scaling，最大程度实现语音与文本能力深度融合和能力涌现。在后训练阶段，团队使用了高质量数据与RL算法，进一步提供模型高情商对话能力与安全性，并使大模型同时兼具智商”与情商”。架构支持多模态输入输出，涌现出较强声音控制、声音扮演等能力在架构方面，豆包团队研发端到端框架，深度融合语音与文本模态，该框架面向语音生成和理解进行统一联合建模，实现多模态输入和输出效果，涵盖S2S（语音到语音）、S2T（语音到文本）、T2S（文本到语音）、T2T（文本到文本）等多种模式。模型涌现多种能力：在声音控制方面，模型不仅能依照基础指令输出，还可遵循丰富的复杂指令；在声音扮演方面，目前模型部分方言和口音，主要源自于预训练阶段数据泛化，而非针对性训练。此外，模型支持实时联网功能，能够根据问题，动态获取最新信息，问题回答时效性强。从测评结来看，豆包实时语音大模型在情绪理解和情感表达方面优势明显，整体满意度方面，豆包实时语音大模型评分为4.36，GPT-4o为3.18，豆包模型表现更优。ToB和ToC端大模型齐飞，重视字节跳动等相关产业链投资机会ToB和ToC端大模型齐飞，建议重视国产算力产业链：（1）【国产算力芯片】推荐标的：中兴通讯；受益标的：寒武纪、海光信息等；（2）【字节/阿里/腾讯/百度等AIDC供应商】推荐标的：润泽科技、宝信软件；受益标的：光环新网、世纪互联、大位科技、东方国信、云赛智联、奥飞数据、万国数据、科华数据等；（3）【液冷】推荐标的：英维克；受益标的：申菱环境、同飞股份、网宿科技、科华数据、高澜股份、申菱环境、依米康、飞荣达等；（4）【服务器电源】受益标的：欧陆通、麦格米特等；（5）【柴油发电机】受益标的：科泰电源、潍柴重机等；（6）【变压器】受益标的：金盘科技等；（7）【铜连接】受益标的：'}\n",
      "{'name': '中文大模型基准测评2024年10月报告-2024年度中文大模型阶段性进展评估', 'url': 'https://www.sohu.com/a/826962523_121825883', 'snippet': '2024年10月的SuperCLUE报告聚焦于中文大模型的最新进展。报告指出，OpenAI的o1-preview在全球大模型竞争中领先，国内模型如Qwen2.5-72B-Instruct和DeepSeek V2.5在全球开源模型中表现突出。\\n国内闭源模型GLM-4-Plus、SenseChat 5.5、AndesGPT-2.0与国际模型差距缩小。报告还强调了端侧小模型的快速发展，特别是在中文场景下的应用潜力。\\n报告目录：\\n报告核心结论摘要\\n国内大模型关键进展及趋势\\nSuperCLUE通用能力测评\\nSuperCLUE专项与行业基准测评\\nSuperCLUE多模态能力测评\\n精准量化通用人工智能（AGI）进展，定义人类迈向AGI的路线图\\n报告部分内容节选如下：\\n返回搜狐，查看更多\\n责任编辑：'}\n",
      "{'name': '首个可保留情感的音频LLM！Meta重磅开源7B-Spirit LM，一网打尽「音频+文本」多模态任务', 'url': 'https://www.163.com/dy/article/JHJO9R3I0511ABV6.html', 'snippet': '新智元报道\\n编辑：LRS\\n【新智元导读】Meta最近开源了一个7B尺寸的Spirit LM的多模态语言模型，能够理解和生成语音及文本，可以非常自然地在两种模式间转换，不仅能处理基本的语音转文本和文本转语音任务，还能捕捉和再现语音中的情感和风格。\\n在纯文本大模型取得进展的同时，其他模态数据，如语音与文本结合的语言模型（SpeechLMs）也成为了一个热门的研究领域，但现有的模型要么在仅包含语音的数据上进行训练，要么是关注特定任务，如文本转语音（TTS）、自动语音识别（ASR）或翻译，在其他模态数据和任务上的泛化能力十分有限。\\n在大型语言模型（LLM）性能不断提升的情况下，一个常用的方法是先用ASR模型将语音转录成文本，然后用文本模型来生成新的文本，最后再用TTS模型将文本转换成语音，这种流程的一个显著缺陷就是语音表达性不佳，语言模型无法建模并生成富有表现力的语音数据。\\n最近，Meta开源了一个基础多模态语言模型Spirit LM，基于一个70亿参数的预训练文本语言模型，交错使用文本和语音数据进行训练，使模型能够自由地混合文本和语音，在任一模态中生成语言内容。\\n项目主页：https://speechbot.github.io/spiritlm/\\n论文链接：https://arxiv.org/pdf/2402.05755\\n代码链接：https://github.com/facebookresearch/spiritlm\\n开源链接：https://huggingface.co/spirit-lm/Meta-spirit-lm\\n将语音和文本序列拼接成一条token流，并使用一个小型的、自动整理（automatically-curated）的语音-文本平行语料库，采用逐词交错的方法进行训练。\\nSpirit LM有两个版本：基础版（Base）使用语音音素单元（HuBERT），表达版（Expressive）还额外使用音高和风格单元来模拟表达性，以增强模型在生成语音时的表现力，也就是说模型不仅能够理解和生成基本的语音和文本，还能在表达情感和风格方面表现得更加丰富和自然。\\n对于两个版本的模型，文本都使用子词BPE标记进行编码，最终得到的模型既展现了文本模型的语义能力，也具备语音模型的表达能力；模型还能够在少量样本的情况下跨模态学习新任务（例如自动语音识别、文本转语音、语音分类）。\\n不过需要注意的是，和其他预训练模型一样，Sprit LM也可能会生成一些不安全的内容，'}\n",
      "{'name': '腾讯混元上线文生视频并开源，120秒内成片！还有提示词建议', 'url': 'https://www.sohu.com/a/832903527_115978', 'snippet': '智东西\\n作者  汪越\\n编辑  漠影\\n智东西12月3日报道，今天，腾讯混元大模型正式上线视频生成能力，这是在腾讯文生文、文生图、3D生成之后的最新技术进展。\\n据腾讯混元多模态生成技术负责人凯撒现场介绍，此次更新中，HunYuan-Video模型经历了四项核心改进：\\n1、引入超大规模数据处理系统，提升视频画质；\\n2、采用多模态大语言模型（MLLM），优化文本与图像的对齐；\\n3、使用130亿参数的全注意力机制（DIT）和双模态ScalingLaw，增强时空建模与动态表现；\\n4、采用自研3D VAE架构，提升图像和视频的重建能力。\\n与此同时，腾讯宣布将这款拥有130亿参数规模的视频生成模型开源。目前，该模型已在APP与Web端发布，其标准模式下的视频生成大约需要120秒完成。\\n一、腾讯HunYuan-Video模型技术升级与应用拓展\\n腾讯对HunYuan-Video模型进行了四项技术升级，涵盖了数据处理系统、文本编码、算力优化等多个方面，提升了视频生成的质量与可控性。此外，腾讯还通过微调、应用拓展及开源等措施进一步强化了模型的实际应用能力。\\n1、四项关键技术升级\\n首先，模型采用了一个超大规模的数据处理系统，能够混合处理图像与视频数据。该系统包括文字检测、转景检测、美学打分、动作检测、准确度检测等多个维度的功能，进一步提升视频画质。\\n其次，模型引入了多模态大语言模型（Decoder-only MLLM）作为文本编码器，提升了复杂文本的理解能力，同时支持多语言理解。这一升级使得文本与图像之间的对齐性得到了加强，能够根据用户提供的提示词精确生成符合要求的视频内容。\\n另外，模型架构使用了130亿参数的全注意力机制（DIT）和双模态ScalingLaw，能够在视频生成中有效利用算力和数据资源，增强时空建模能力，并优化视频生成过程中的动态表现。此架构支持原生转场，可实现了多个镜头间的自然切换，并保持主体一致性。\\n最后，HunYuan-Video采用了自研的3D VAE架构，以提升图像和视频重建的能力，特别在小人脸和大幅运动场景下表现更加流畅。\\n2、六大微调领域强化定向能力\\n在预训练之后，腾讯混元大模型目前正在进行微调（SFT）工作，进一步增强其视频生成的定向能力。HunYuan-Video在六个关键方面进行了专项微调，包括画质优化、高动态效果、艺术镜头、手写文本、转场效果以及连续动作的生成，其中一些调整仍在进行中。\\n3、Recaption模型与两种生成模式'}\n",
      "{'name': '智源研究院与腾讯达成战略合作，推动大模型技术前沿探索和应用落地', 'url': 'https://blog.csdn.net/eagleofstar/article/details/144589920', 'snippet': '2024 年 12 月 18日， 智源研究院与腾讯签署战略合作协议，双方将在大模型研发、人工智能技术前沿探索及开源生态建设等领域展开深入合作。智源研究院院长王仲远、副院长兼总工程师林咏华，腾讯集团高级执行副总裁、云与智慧产业事业群总裁汤道生、腾讯云副总裁、腾讯教育负责人石梅等出席签约仪式。\\n此次战略合作，双方将充分发挥各自优势，共同推动大模型技术与产业场景深度融合，探索多元算力下的大模型训练与推理优化技术解决方案，构建开放创新的软硬件生态体系等。\\n智源研究院院长王仲远在签约仪式上表示，智源研究院是专注于人工智能技术原始创新的新型研发机构，持续引领大模型的技术发展方向，布局多模态世界模型、具身智能、AI for Science 等前沿领域并取得了一系列的国际领先的科研成果与进展；同时，推出了包括模型、数据、算法、评测、系统的大模型全栈开源技术基座以及面向大模型、支持多种异构算力的智算集群软件栈，以赋能产业应用与技术生态的发展。此次，智源研究院携手腾讯，将促进产研协同发展，推动技术转化与产业落地，形成从实验室研究到赋能千行百业的技术生态”。\\n腾讯集团高级执行副总裁，云与智慧产业事业群CEO 汤道生表示，腾讯致力于打造离产业最近的AI，将AI融入到产业场景中。希望腾讯充分发挥在云计算、大数据、人工智能等领域的技术积累，与智源人工智能研究院的研究成果相互融合，共同探索大模型技术的新应用、新场景，探索人工智能技术的无限潜力。'}\n"
     ]
    }
   ],
   "source": [
    "# step 1: 基于智谱AI提供的搜索工具搜索网络内容\n",
    "import requests\n",
    "import uuid\n",
    "# https://open.bigmodel.cn/dev/api/search-tool/web-search-pro\n",
    "def search_with_searchpro(query: str, api_key: str = None):\n",
    "    try:\n",
    "        api_key = api_key or os.environ.get(\"ZHIPUAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise RuntimeError(\"ZHIPUAI_API_KEY not set\")\n",
    "        logger.info(f\"Searching web for: {query}\")\n",
    "        msg = [{\"role\": \"user\", \"content\": query}]\n",
    "        data = {\n",
    "            \"request_id\": str(uuid.uuid4()),\n",
    "            \"tool\": \"web-search-pro\",\n",
    "            \"stream\": False,\n",
    "            \"messages\": msg\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            url=SEARCHPRO_ENDPOINT,\n",
    "            json=data,\n",
    "            headers={'Authorization': api_key},\n",
    "            timeout=120 # s\n",
    "            )\n",
    "        print(response)\n",
    "        \n",
    "\n",
    "        json_content = response.json()\n",
    "        try:\n",
    "            search_results = json_content['choices'][0]['message']['tool_calls'][1]['search_result']\n",
    "            contexts = []\n",
    "\n",
    "            for result in search_results[:REFERENCE_COUNT]:\n",
    "                contexts.append({\n",
    "                    \"name\": result.get(\"title\", \"\"),\n",
    "                    \"url\": result.get(\"link\", \"\"),\n",
    "                    \"snippet\": result.get(\"content\", \"\")\n",
    "                })\n",
    "            logger.debug(f\"Search results: {contexts}\")\n",
    "            return contexts\n",
    "        except KeyError:\n",
    "            logger.error(f\"Error encountered: {json_content}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to search with SearchPro: {e}\")\n",
    "        return []\n",
    "\n",
    "SEARCHPRO_ENDPOINT = \"https://open.bigmodel.cn/api/paas/v4/tools\"\n",
    "\n",
    "REFERENCE_COUNT = 10\n",
    "DEFAULT_SEARCH_ENGINE_TIMEOUT = 5\n",
    "ZHIPUAI_API_KEY='4cbbdb4079ce50378af38685e7927e3f.pF2jBgOkUji8KgKd'\n",
    "api_key = ZHIPUAI_API_KEY\n",
    "search_function = lambda q: search_with_searchpro(q, api_key)\n",
    "q = '开源多模态大模型最新进展?'\n",
    "contexts=search_function(q)    \n",
    "for x in contexts:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-22 10:08:17.846\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_answer\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mmessages: [{'role': 'system', 'content': '你是一个大型的语言AI助手。当用户提出问题时，请你写出清晰、简洁且准确的答案。我们会给你一组与问题相关的上下文，每个上下文都以类似[[citation:x]]这样的引用编号开始，其中x是一个数字。如果有引用[context]，请在每句话后面使用并引述该上下文。\\n\\n你的答案必须正确、精确，并由专家以公正和专业的语气撰写。请将你的回答限制在1024个token内。如果所提供的上下文信息不足，可以使用自己知识来回答用户问题。\\n\\n请按照[citation:x]格式引用带有参考编号的上下文。如果一句话来自多个上下文，请列出所有适用于此处引述，如[citation:3][citation:5]。除代码、特定名称和引述外，你必须使用与问题相同语言编写你的回答。\\n'}, {'role': 'user', 'content': '[context]=```\\n[[citation:1]] 摘要\\nMiniMax，选择用开源「震撼」一下全球同行。\\n作者  Li Yuan\\n编辑  郑玄\\n大模型时代已经正式迈入第三年。\\n回首过去的两年，不禁令人感慨。每年都有基座大模型架构已经尘埃落定的声音，然而每年，技术都在飞快地迭代创新，突破人们想象。\\n2024 年，OpenAI 的推理模型，通过对模型架构的创新，用 RL 的方法延续 Scaling Law，让大模型的智力水平持续进展；而中国公司也并没有落后，价格屠夫 DeepSeek 通过 MLA 的架构创新，让推理成本直接降低了一个数量级。\\n2025 年开年，令人欣喜的是，我们看到了一向在人们印象中是「低调做产品」的 MiniMax 公司，也加入了开源行列，将最先进的底层技术直接与社区和行业分享。\\n1 月 15 日，大模型公司 MiniMax 正式发布了 MiniMax-01 系列模型。它包括基础语言大模型 MiniMax-Text-01，和在其上集成了一个轻量级 ViT 模型而开发的视觉多模态大模型 MiniMax-VL-01。\\n「卷」起来的大模型公司，令人乐见。开源会提升创新效率，越来越好的基座模型之上，才搭建越来越有用的应用，进入千家万户，帮人们解放生产力。\\n这是 MiniMax 第一次发布开源模型，一出手就是一个炸裂模型架构创新：新模型采用了 MiniMax 独有的 Lightning Attention 机制，借鉴了 Linear Attention（线性注意力）机制，是全球第一次将 Linear Attention 机制引入到商业化规模的模型当中。\\n效果也是立竿见影，模型上下文长度直接达到了顶尖模型的 20-32 倍水平，推理时的上下文窗口能达到 400 万 token。模型效果立刻在海外上引起了关注。\\n模型的上下文窗口，指的是模型在生成每个新 token 时，实际参考的前面内容的范围。就像是模型能够一次性从书架上取下的书籍数量。模型的上下文窗口越大，模型生成时可以参考的信息量就越多，表现也就更加智能。\\n站在 2025 年年初的时间点，长上下文窗口还有一个新的意义：为模型的 Agent 能力，打下坚实基础。\\n业界公认，2025 年，Agent 能力将是 AI 届「卷生卷死」的重点，连 OpenAI 都在本周内推出了 Tasks，一个 AI Agent 的雏型。在 2025 年，我们很有可能看到越来越多真正「全自动的」AI，在我们的生活中起作用。甚至不同「\\n\\n[[citation:2]] MiniMax，选择用开源「震撼」一下全球同行。\\n大模型时代已经正式迈入第三年。\\n回首过去的两年，不禁令人感慨。每年都有基座大模型架构已经尘埃落定的声音，然而每年，技术都在飞快地迭代创新，突破人们想象。\\n2024 年，OpenAI 的推理模型，通过对模型架构的创新，用 RL 的方法延续 Scaling Law，让大模型的智力水平持续进展；而中国公司也并没有落后，价格屠夫 DeepSeek 通过 MLA 的架构创新，让推理成本直接降低了一个数量级。\\n2025 年开年，令人欣喜的是，我们看到了一向在人们印象中是「低调做产品」的 MiniMax 公司，也加入了开源行列，将最先进的底层技术直接与社区和行业分享。\\n1 月 15 日，大模型公司 MiniMax 正式发布了 MiniMax-01 系列模型。它包括基础语言大模型 MiniMax-Text-01，和在其上集成了一个轻量级 ViT 模型而开发的视觉多模态大模型 MiniMax-VL-01。\\n「卷」起来的大模型公司，令人乐见。开源会提升创新效率，越来越好的基座模型之上，才搭建越来越有用的应用，进入千家万户，帮人们解放生产力。\\n这是 MiniMax 第一次发布开源模型，一出手就是一个炸裂模型架构创新：新模型采用了 MiniMax 独有的 Lightening Attention 机制，借鉴了 Linear Attention（线性注意力）机制，是全球第一次将 Linear Attention 机制引入到商业化规模的模型当中。\\n效果也是立竿见影，模型上下文长度直接达到了顶尖模型的 20-32 倍水平，推理时的上下文窗口能达到 400 万 token。模型效果立刻在海外上引起了关注。\\n模型的上下文窗口，指的是模型在生成每个新 token 时，实际参考的前面内容的范围。就像是模型能够一次性从书架上取下的书籍数量。模型的上下文窗口越大，模型生成时可以参考的信息量就越多，表现也就更加智能。\\n站在 2025 年年初的时间点，长上下文窗口还有一个新的意义：为模型的 Agent 能力，打下坚实基础。\\n业界公认，2025 年，Agent 能力将是 AI 届「卷生卷死」的重点，连 OpenAI 都在本周内推出了 Tasks，一个 AI Agent 的雏型。在 2025 年，我们很有可能看到越来越多真正「全自动的」AI，在我们的生活中起作用。甚至不同「全自动的」的 AI 共同协作，帮我们处理事务。\\n\\n[[citation:3]] 「2025 年，我们可能会看到第一批 AI Agent 加入劳动力大军，并对公司的生产力产生实质性的影响。」——OpenAI CEO Sam Altman\\n「2025 年，每个公司都将拥有 AI 软件工程师 Agent，它们会编写大量代码。」——Meta CEO Mark Zuckerberg\\n「未来，每家公司的 IT 部门都将成为 AI Agent 的 HR 部门。」—— 英伟达 CEO 黄仁勋\\n2025 新年伊始，在很多趋势都还不明朗的情况下，几位 AI 业界的重要人物几乎在同一时间做出了类似的判断 ——2025 年将是 AI Agent 之年。\\n没想到，MiniMax 很快就有了动作：开源了最新的基础语言模型 MiniMax-Text-01 和视觉多模态模型 MiniMax-VL-01。\\n新模型的最大亮点是，在业内首次大规模实现了新的线性注意力机制，这使得输入的上下文窗口大大变长：一次可处理 400 万 token，是其他模型的 20-32 倍。\\n他们相信，这些模型能够给接下来一年潜在 Agent 相关应用的爆发做出贡献。\\n为什么这项工作对于 Agent 如此重要？\\n随着 Agent 进入应用场景，无论是单个 Agent 工作时产生的记忆，还是多个 Agent 协作所产生的 context，都会对模型的长上下文窗口提出更多需求。\\n开源地址：https://github.com/MiniMax-AI\\nHugging Face：https://huggingface.co/MiniMaxAI\\n技术报告：https://filecdn.minimax.chat/ArxivMiniMax01Report.pdf\\nAPI：https://www.minimaxi.com/platform\\n一系列创新 造就比肩顶尖模型的开源模型 MiniMax-Text-01 究竟是如何炼成的？事实上，他们为此进行了一系列创新。从新型线性注意力到改进版混合专家架构，再到并行策略和通信技术的优化，MiniMax 解决了大模型在面对超长上下文时的多项效果与效率痛点。\\nLightning Attention 目前领先的 LLM 大都基于 Transformer，而 Transformer 核心的自注意力机制是其计算成本的重要来源。为了优化，研究社区可以说是绞尽脑汁，提出了稀疏注意力、低秩分解和线性注意力等许多技术。MiniMax 的 Lightning Attention 便是一种线性注意力。\\n\\n[[citation:4]] 今天分享的是：大模型专题：多模态大语言模型领域进展分享\\n报告共计：42页\\n多模态大语言模型（MLLM）兴起，能处理多模态任务，如视觉问答、图表推理等。其架构包含编码器、连接器和大语言模型，数据训练分模态对齐和指令微调两阶段，评估方式多样。模型演进体现在分辨率提高、输入形式丰富、I/O模态支持增加，如支持多图视频输入、输出图文交错内容等。团队工作包括缓解幻觉，利用外部反馈和视觉基础模型提升准确性；构建长视频理解测评基准，发现开源模型有提升空间且字幕对理解有正向作用；提升多模态交互体验，如VITA无需唤醒词、支持打断输出和屏蔽噪声，其实现包括特殊状态token和双工机制。未来，MLLM需支持更长上下文、具备多模态Agent、实现轻量化部署、统一多模态生成与理解及训练和推理流程，以应对视觉token增多、长视频理解等需求，推动模型发展。\\n以下为报告节选内容\\n返回搜狐，查看更多\\n责任编辑：\\n\\n[[citation:5]] 2025，见证更多GenAI应用落实！今年的中国AIGC产业峰会来啦\\n一水\\n量子位\\n仅一天时间，就能让你提前洞悉2025年AI发展趋势\\n过去一年，我们见证了AI无数个里程碑式”时刻。\\n从Sora带来的超写实视频生成能力，到开源大模型在性能上逐渐追平闭源模型；从多模态理解能力质的飞跃，到Agent技术在实际应用中的突破性进展\\n甚至就连AGI（通用人工智能）这种听起来遥不可及的词汇，也在大佬们的预测中逐渐接近。\\n无怪乎全网直呼，简直不敢想象2025年AI将如何发展？\\n此情此景下，一场由各行各业AI玩家代表总结过去、布局未来的大会，就这样应呼声而来。\\n仅一天时间，就能让你提前洞悉2025年AI发展趋势。\\n4月16日，我们将在北京举办第三届中国AIGC产业峰会，本届峰会主题为「万物皆可AI」。\\n届时我们将邀请更多AI应用层玩家，和AI基础设施、模型层的代表玩家，一同来分享对最新生成式AI现状与趋势的见解。\\n在往届的峰会上，我们邀请到了科技领域头部企业、AIGC垂直代表公司高管百度袁佛玉、微软大中华区关玮雅、美图CEO吴欣鸿、小冰COO徐元春等，科研领域的学者大咖智源研究院林咏华、人大高瓴卢志武教授等嘉宾分享交流。\\n峰会中有近千观众线下参会，300万观众观看线上直播，全网曝光2000万，获得了主流媒体的广泛关注与报道。\\n今年，我们诚邀你继续一同见证更多AI应用落地，并一探生成式AI的未来！\\n万物皆可AI的时代已经到来。\\n随着大模型浪潮深入，模型能力更强，应用成本更低，各行各业新产品新物种层出不穷。智能助手已经寻常，AI陪伴如火如荼，代码生成准备就绪，视频生成越来越强，生产力和互动娱乐交相辉映，软件硬件一浪高过一浪。\\n还有不用AI的吗？还有没让AI赋能的吗？万物还能怎样AI起来？\\n2025年AIGC产业峰会，我们以「万物皆可AI」为主题，聚焦「万物皆可AI」，诚邀AI创业者、开发者、资深使用者，让更多的AI落地被看见，让更多人用上AI、用好AI，被AI加速成长。\\n欢迎来到，「万物皆可AI」时代。\\n过去一年里AIGC产业风起云涌，无数优秀的企业、产品涌现而出，量子位将根据过去一年里AIGC企业、产品的表现与反馈，结合对2024年技术与场景的观察，评选出2025年值得关注的AIGC企业、 2025年值得关注的AIGC产品两类奖项。\\n产品创新力关注产品在功能、体验和应用场景方面的创新性和独特性；\\n本次评选即日起开始报名，截至2025年3月24日，\\n\\n[[citation:6]] （以下内容从开源证券《通信行业点评报告：字节发布豆包实时语音模型，重视AI产业链》研报附件原文摘录）字节发布豆包实时语音大模型发布，AI版图持续扩张2025年1月20日，字节跳动发布豆包实时语音大模型，该模型是一款语音理解和生成一体化的模型，实现了端到端语音对话。相比传统级联模式，该模型在语音表现力、控制力、情绪承接方面表现较强，并具备低时延、对话中可随时打断等特性。在数据收集中，团队筛选并整理大量包含丰富情感的语音数据，涵盖各种场景与情绪状态。在预训练阶段，团队对各模态交织数据进行深入训练，精准捕捉并高效压缩海量语音信息，通过Scaling，最大程度实现语音与文本能力深度融合和能力涌现。在后训练阶段，团队使用了高质量数据与RL算法，进一步提供模型高情商对话能力与安全性，并使大模型同时兼具智商”与情商”。架构支持多模态输入输出，涌现出较强声音控制、声音扮演等能力在架构方面，豆包团队研发端到端框架，深度融合语音与文本模态，该框架面向语音生成和理解进行统一联合建模，实现多模态输入和输出效果，涵盖S2S（语音到语音）、S2T（语音到文本）、T2S（文本到语音）、T2T（文本到文本）等多种模式。模型涌现多种能力：在声音控制方面，模型不仅能依照基础指令输出，还可遵循丰富的复杂指令；在声音扮演方面，目前模型部分方言和口音，主要源自于预训练阶段数据泛化，而非针对性训练。此外，模型支持实时联网功能，能够根据问题，动态获取最新信息，问题回答时效性强。从测评结来看，豆包实时语音大模型在情绪理解和情感表达方面优势明显，整体满意度方面，豆包实时语音大模型评分为4.36，GPT-4o为3.18，豆包模型表现更优。ToB和ToC端大模型齐飞，重视字节跳动等相关产业链投资机会ToB和ToC端大模型齐飞，建议重视国产算力产业链：（1）【国产算力芯片】推荐标的：中兴通讯；受益标的：寒武纪、海光信息等；（2）【字节/阿里/腾讯/百度等AIDC供应商】推荐标的：润泽科技、宝信软件；受益标的：光环新网、世纪互联、大位科技、东方国信、云赛智联、奥飞数据、万国数据、科华数据等；（3）【液冷】推荐标的：英维克；受益标的：申菱环境、同飞股份、网宿科技、科华数据、高澜股份、申菱环境、依米康、飞荣达等；（4）【服务器电源】受益标的：欧陆通、麦格米特等；（5）【柴油发电机】受益标的：科泰电源、潍柴重机等；（6）【变压器】受益标的：金盘科技等；（7）【铜连接】受益标的：\\n\\n[[citation:7]] 2024年10月的SuperCLUE报告聚焦于中文大模型的最新进展。报告指出，OpenAI的o1-preview在全球大模型竞争中领先，国内模型如Qwen2.5-72B-Instruct和DeepSeek V2.5在全球开源模型中表现突出。\\n国内闭源模型GLM-4-Plus、SenseChat 5.5、AndesGPT-2.0与国际模型差距缩小。报告还强调了端侧小模型的快速发展，特别是在中文场景下的应用潜力。\\n报告目录：\\n报告核心结论摘要\\n国内大模型关键进展及趋势\\nSuperCLUE通用能力测评\\nSuperCLUE专项与行业基准测评\\nSuperCLUE多模态能力测评\\n精准量化通用人工智能（AGI）进展，定义人类迈向AGI的路线图\\n报告部分内容节选如下：\\n返回搜狐，查看更多\\n责任编辑：\\n\\n[[citation:8]] 新智元报道\\n编辑：LRS\\n【新智元导读】Meta最近开源了一个7B尺寸的Spirit LM的多模态语言模型，能够理解和生成语音及文本，可以非常自然地在两种模式间转换，不仅能处理基本的语音转文本和文本转语音任务，还能捕捉和再现语音中的情感和风格。\\n在纯文本大模型取得进展的同时，其他模态数据，如语音与文本结合的语言模型（SpeechLMs）也成为了一个热门的研究领域，但现有的模型要么在仅包含语音的数据上进行训练，要么是关注特定任务，如文本转语音（TTS）、自动语音识别（ASR）或翻译，在其他模态数据和任务上的泛化能力十分有限。\\n在大型语言模型（LLM）性能不断提升的情况下，一个常用的方法是先用ASR模型将语音转录成文本，然后用文本模型来生成新的文本，最后再用TTS模型将文本转换成语音，这种流程的一个显著缺陷就是语音表达性不佳，语言模型无法建模并生成富有表现力的语音数据。\\n最近，Meta开源了一个基础多模态语言模型Spirit LM，基于一个70亿参数的预训练文本语言模型，交错使用文本和语音数据进行训练，使模型能够自由地混合文本和语音，在任一模态中生成语言内容。\\n项目主页：https://speechbot.github.io/spiritlm/\\n论文链接：https://arxiv.org/pdf/2402.05755\\n代码链接：https://github.com/facebookresearch/spiritlm\\n开源链接：https://huggingface.co/spirit-lm/Meta-spirit-lm\\n将语音和文本序列拼接成一条token流，并使用一个小型的、自动整理（automatically-curated）的语音-文本平行语料库，采用逐词交错的方法进行训练。\\nSpirit LM有两个版本：基础版（Base）使用语音音素单元（HuBERT），表达版（Expressive）还额外使用音高和风格单元来模拟表达性，以增强模型在生成语音时的表现力，也就是说模型不仅能够理解和生成基本的语音和文本，还能在表达情感和风格方面表现得更加丰富和自然。\\n对于两个版本的模型，文本都使用子词BPE标记进行编码，最终得到的模型既展现了文本模型的语义能力，也具备语音模型的表达能力；模型还能够在少量样本的情况下跨模态学习新任务（例如自动语音识别、文本转语音、语音分类）。\\n不过需要注意的是，和其他预训练模型一样，Sprit LM也可能会生成一些不安全的内容，\\n\\n[[citation:9]] 智东西\\n作者  汪越\\n编辑  漠影\\n智东西12月3日报道，今天，腾讯混元大模型正式上线视频生成能力，这是在腾讯文生文、文生图、3D生成之后的最新技术进展。\\n据腾讯混元多模态生成技术负责人凯撒现场介绍，此次更新中，HunYuan-Video模型经历了四项核心改进：\\n1、引入超大规模数据处理系统，提升视频画质；\\n2、采用多模态大语言模型（MLLM），优化文本与图像的对齐；\\n3、使用130亿参数的全注意力机制（DIT）和双模态ScalingLaw，增强时空建模与动态表现；\\n4、采用自研3D VAE架构，提升图像和视频的重建能力。\\n与此同时，腾讯宣布将这款拥有130亿参数规模的视频生成模型开源。目前，该模型已在APP与Web端发布，其标准模式下的视频生成大约需要120秒完成。\\n一、腾讯HunYuan-Video模型技术升级与应用拓展\\n腾讯对HunYuan-Video模型进行了四项技术升级，涵盖了数据处理系统、文本编码、算力优化等多个方面，提升了视频生成的质量与可控性。此外，腾讯还通过微调、应用拓展及开源等措施进一步强化了模型的实际应用能力。\\n1、四项关键技术升级\\n首先，模型采用了一个超大规模的数据处理系统，能够混合处理图像与视频数据。该系统包括文字检测、转景检测、美学打分、动作检测、准确度检测等多个维度的功能，进一步提升视频画质。\\n其次，模型引入了多模态大语言模型（Decoder-only MLLM）作为文本编码器，提升了复杂文本的理解能力，同时支持多语言理解。这一升级使得文本与图像之间的对齐性得到了加强，能够根据用户提供的提示词精确生成符合要求的视频内容。\\n另外，模型架构使用了130亿参数的全注意力机制（DIT）和双模态ScalingLaw，能够在视频生成中有效利用算力和数据资源，增强时空建模能力，并优化视频生成过程中的动态表现。此架构支持原生转场，可实现了多个镜头间的自然切换，并保持主体一致性。\\n最后，HunYuan-Video采用了自研的3D VAE架构，以提升图像和视频重建的能力，特别在小人脸和大幅运动场景下表现更加流畅。\\n2、六大微调领域强化定向能力\\n在预训练之后，腾讯混元大模型目前正在进行微调（SFT）工作，进一步增强其视频生成的定向能力。HunYuan-Video在六个关键方面进行了专项微调，包括画质优化、高动态效果、艺术镜头、手写文本、转场效果以及连续动作的生成，其中一些调整仍在进行中。\\n3、Recaption模型与两种生成模式\\n\\n[[citation:10]] 2024 年 12 月 18日， 智源研究院与腾讯签署战略合作协议，双方将在大模型研发、人工智能技术前沿探索及开源生态建设等领域展开深入合作。智源研究院院长王仲远、副院长兼总工程师林咏华，腾讯集团高级执行副总裁、云与智慧产业事业群总裁汤道生、腾讯云副总裁、腾讯教育负责人石梅等出席签约仪式。\\n此次战略合作，双方将充分发挥各自优势，共同推动大模型技术与产业场景深度融合，探索多元算力下的大模型训练与推理优化技术解决方案，构建开放创新的软硬件生态体系等。\\n智源研究院院长王仲远在签约仪式上表示，智源研究院是专注于人工智能技术原始创新的新型研发机构，持续引领大模型的技术发展方向，布局多模态世界模型、具身智能、AI for Science 等前沿领域并取得了一系列的国际领先的科研成果与进展；同时，推出了包括模型、数据、算法、评测、系统的大模型全栈开源技术基座以及面向大模型、支持多种异构算力的智算集群软件栈，以赋能产业应用与技术生态的发展。此次，智源研究院携手腾讯，将促进产研协同发展，推动技术转化与产业落地，形成从实验室研究到赋能千行百业的技术生态”。\\n腾讯集团高级执行副总裁，云与智慧产业事业群CEO 汤道生表示，腾讯致力于打造离产业最近的AI，将AI融入到产业场景中。希望腾讯充分发挥在云计算、大数据、人工智能等领域的技术积累，与智源人工智能研究院的研究成果相互融合，共同探索大模型技术的新应用、新场景，探索人工智能技术的无限潜力。\\n```\\n当前日期: 2025-01-22\\n\\n基于上下文回答问题，不要盲目地逐字重复上下文。请以[citation:x]的格式引用上下文。这是用户的问题：\\n\\n\\n开源多模态大模型最新进展?'}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '开源多模态大模型最新进展?', 'contexts': [{'name': '只有模仿没有创新？国产 AI 用行动打破质疑', 'url': 'https://finance.sina.com.cn/tech/roll/2025-01-17/doc-ineffyrh3466257.shtml', 'snippet': '摘要\\nMiniMax，选择用开源「震撼」一下全球同行。\\n作者  Li Yuan\\n编辑  郑玄\\n大模型时代已经正式迈入第三年。\\n回首过去的两年，不禁令人感慨。每年都有基座大模型架构已经尘埃落定的声音，然而每年，技术都在飞快地迭代创新，突破人们想象。\\n2024 年，OpenAI 的推理模型，通过对模型架构的创新，用 RL 的方法延续 Scaling Law，让大模型的智力水平持续进展；而中国公司也并没有落后，价格屠夫 DeepSeek 通过 MLA 的架构创新，让推理成本直接降低了一个数量级。\\n2025 年开年，令人欣喜的是，我们看到了一向在人们印象中是「低调做产品」的 MiniMax 公司，也加入了开源行列，将最先进的底层技术直接与社区和行业分享。\\n1 月 15 日，大模型公司 MiniMax 正式发布了 MiniMax-01 系列模型。它包括基础语言大模型 MiniMax-Text-01，和在其上集成了一个轻量级 ViT 模型而开发的视觉多模态大模型 MiniMax-VL-01。\\n「卷」起来的大模型公司，令人乐见。开源会提升创新效率，越来越好的基座模型之上，才搭建越来越有用的应用，进入千家万户，帮人们解放生产力。\\n这是 MiniMax 第一次发布开源模型，一出手就是一个炸裂模型架构创新：新模型采用了 MiniMax 独有的 Lightning Attention 机制，借鉴了 Linear Attention（线性注意力）机制，是全球第一次将 Linear Attention 机制引入到商业化规模的模型当中。\\n效果也是立竿见影，模型上下文长度直接达到了顶尖模型的 20-32 倍水平，推理时的上下文窗口能达到 400 万 token。模型效果立刻在海外上引起了关注。\\n模型的上下文窗口，指的是模型在生成每个新 token 时，实际参考的前面内容的范围。就像是模型能够一次性从书架上取下的书籍数量。模型的上下文窗口越大，模型生成时可以参考的信息量就越多，表现也就更加智能。\\n站在 2025 年年初的时间点，长上下文窗口还有一个新的意义：为模型的 Agent 能力，打下坚实基础。\\n业界公认，2025 年，Agent 能力将是 AI 届「卷生卷死」的重点，连 OpenAI 都在本周内推出了 Tasks，一个 AI Agent 的雏型。在 2025 年，我们很有可能看到越来越多真正「全自动的」AI，在我们的生活中起作用。甚至不同「'}, {'name': '只有模仿没有创新？国产 AI 用行动打破质疑', 'url': 'https://finance.jrj.com.cn/2025/01/17115247543522.shtml', 'snippet': 'MiniMax，选择用开源「震撼」一下全球同行。\\n大模型时代已经正式迈入第三年。\\n回首过去的两年，不禁令人感慨。每年都有基座大模型架构已经尘埃落定的声音，然而每年，技术都在飞快地迭代创新，突破人们想象。\\n2024 年，OpenAI 的推理模型，通过对模型架构的创新，用 RL 的方法延续 Scaling Law，让大模型的智力水平持续进展；而中国公司也并没有落后，价格屠夫 DeepSeek 通过 MLA 的架构创新，让推理成本直接降低了一个数量级。\\n2025 年开年，令人欣喜的是，我们看到了一向在人们印象中是「低调做产品」的 MiniMax 公司，也加入了开源行列，将最先进的底层技术直接与社区和行业分享。\\n1 月 15 日，大模型公司 MiniMax 正式发布了 MiniMax-01 系列模型。它包括基础语言大模型 MiniMax-Text-01，和在其上集成了一个轻量级 ViT 模型而开发的视觉多模态大模型 MiniMax-VL-01。\\n「卷」起来的大模型公司，令人乐见。开源会提升创新效率，越来越好的基座模型之上，才搭建越来越有用的应用，进入千家万户，帮人们解放生产力。\\n这是 MiniMax 第一次发布开源模型，一出手就是一个炸裂模型架构创新：新模型采用了 MiniMax 独有的 Lightening Attention 机制，借鉴了 Linear Attention（线性注意力）机制，是全球第一次将 Linear Attention 机制引入到商业化规模的模型当中。\\n效果也是立竿见影，模型上下文长度直接达到了顶尖模型的 20-32 倍水平，推理时的上下文窗口能达到 400 万 token。模型效果立刻在海外上引起了关注。\\n模型的上下文窗口，指的是模型在生成每个新 token 时，实际参考的前面内容的范围。就像是模型能够一次性从书架上取下的书籍数量。模型的上下文窗口越大，模型生成时可以参考的信息量就越多，表现也就更加智能。\\n站在 2025 年年初的时间点，长上下文窗口还有一个新的意义：为模型的 Agent 能力，打下坚实基础。\\n业界公认，2025 年，Agent 能力将是 AI 届「卷生卷死」的重点，连 OpenAI 都在本周内推出了 Tasks，一个 AI Agent 的雏型。在 2025 年，我们很有可能看到越来越多真正「全自动的」AI，在我们的生活中起作用。甚至不同「全自动的」的 AI 共同协作，帮我们处理事务。'}, {'name': 'MiniMax震撼开源，突破传统Transformer架构，4560亿参数，支持400万长上下文', 'url': 'https://www.jiqizhixin.com/articles/2025-01-15-9', 'snippet': '「2025 年，我们可能会看到第一批 AI Agent 加入劳动力大军，并对公司的生产力产生实质性的影响。」——OpenAI CEO Sam Altman\\n「2025 年，每个公司都将拥有 AI 软件工程师 Agent，它们会编写大量代码。」——Meta CEO Mark Zuckerberg\\n「未来，每家公司的 IT 部门都将成为 AI Agent 的 HR 部门。」—— 英伟达 CEO 黄仁勋\\n2025 新年伊始，在很多趋势都还不明朗的情况下，几位 AI 业界的重要人物几乎在同一时间做出了类似的判断 ——2025 年将是 AI Agent 之年。\\n没想到，MiniMax 很快就有了动作：开源了最新的基础语言模型 MiniMax-Text-01 和视觉多模态模型 MiniMax-VL-01。\\n新模型的最大亮点是，在业内首次大规模实现了新的线性注意力机制，这使得输入的上下文窗口大大变长：一次可处理 400 万 token，是其他模型的 20-32 倍。\\n他们相信，这些模型能够给接下来一年潜在 Agent 相关应用的爆发做出贡献。\\n为什么这项工作对于 Agent 如此重要？\\n随着 Agent 进入应用场景，无论是单个 Agent 工作时产生的记忆，还是多个 Agent 协作所产生的 context，都会对模型的长上下文窗口提出更多需求。\\n开源地址：https://github.com/MiniMax-AI\\nHugging Face：https://huggingface.co/MiniMaxAI\\n技术报告：https://filecdn.minimax.chat/ArxivMiniMax01Report.pdf\\nAPI：https://www.minimaxi.com/platform\\n一系列创新 造就比肩顶尖模型的开源模型 MiniMax-Text-01 究竟是如何炼成的？事实上，他们为此进行了一系列创新。从新型线性注意力到改进版混合专家架构，再到并行策略和通信技术的优化，MiniMax 解决了大模型在面对超长上下文时的多项效果与效率痛点。\\nLightning Attention 目前领先的 LLM 大都基于 Transformer，而 Transformer 核心的自注意力机制是其计算成本的重要来源。为了优化，研究社区可以说是绞尽脑汁，提出了稀疏注意力、低秩分解和线性注意力等许多技术。MiniMax 的 Lightning Attention 便是一种线性注意力。'}, {'name': '大模型专题：多模态大语言模型领域进展分享', 'url': 'https://www.sohu.com/a/839539744_121694397', 'snippet': '今天分享的是：大模型专题：多模态大语言模型领域进展分享\\n报告共计：42页\\n多模态大语言模型（MLLM）兴起，能处理多模态任务，如视觉问答、图表推理等。其架构包含编码器、连接器和大语言模型，数据训练分模态对齐和指令微调两阶段，评估方式多样。模型演进体现在分辨率提高、输入形式丰富、I/O模态支持增加，如支持多图视频输入、输出图文交错内容等。团队工作包括缓解幻觉，利用外部反馈和视觉基础模型提升准确性；构建长视频理解测评基准，发现开源模型有提升空间且字幕对理解有正向作用；提升多模态交互体验，如VITA无需唤醒词、支持打断输出和屏蔽噪声，其实现包括特殊状态token和双工机制。未来，MLLM需支持更长上下文、具备多模态Agent、实现轻量化部署、统一多模态生成与理解及训练和推理流程，以应对视觉token增多、长视频理解等需求，推动模型发展。\\n以下为报告节选内容\\n返回搜狐，查看更多\\n责任编辑：'}, {'name': '2025，见证更多GenAI应用落实！今年的中国AIGC产业峰会来啦', 'url': 'https://www.qbitai.com/2025/01/247128.html', 'snippet': '2025，见证更多GenAI应用落实！今年的中国AIGC产业峰会来啦\\n一水\\n量子位\\n仅一天时间，就能让你提前洞悉2025年AI发展趋势\\n过去一年，我们见证了AI无数个里程碑式”时刻。\\n从Sora带来的超写实视频生成能力，到开源大模型在性能上逐渐追平闭源模型；从多模态理解能力质的飞跃，到Agent技术在实际应用中的突破性进展\\n甚至就连AGI（通用人工智能）这种听起来遥不可及的词汇，也在大佬们的预测中逐渐接近。\\n无怪乎全网直呼，简直不敢想象2025年AI将如何发展？\\n此情此景下，一场由各行各业AI玩家代表总结过去、布局未来的大会，就这样应呼声而来。\\n仅一天时间，就能让你提前洞悉2025年AI发展趋势。\\n4月16日，我们将在北京举办第三届中国AIGC产业峰会，本届峰会主题为「万物皆可AI」。\\n届时我们将邀请更多AI应用层玩家，和AI基础设施、模型层的代表玩家，一同来分享对最新生成式AI现状与趋势的见解。\\n在往届的峰会上，我们邀请到了科技领域头部企业、AIGC垂直代表公司高管百度袁佛玉、微软大中华区关玮雅、美图CEO吴欣鸿、小冰COO徐元春等，科研领域的学者大咖智源研究院林咏华、人大高瓴卢志武教授等嘉宾分享交流。\\n峰会中有近千观众线下参会，300万观众观看线上直播，全网曝光2000万，获得了主流媒体的广泛关注与报道。\\n今年，我们诚邀你继续一同见证更多AI应用落地，并一探生成式AI的未来！\\n万物皆可AI的时代已经到来。\\n随着大模型浪潮深入，模型能力更强，应用成本更低，各行各业新产品新物种层出不穷。智能助手已经寻常，AI陪伴如火如荼，代码生成准备就绪，视频生成越来越强，生产力和互动娱乐交相辉映，软件硬件一浪高过一浪。\\n还有不用AI的吗？还有没让AI赋能的吗？万物还能怎样AI起来？\\n2025年AIGC产业峰会，我们以「万物皆可AI」为主题，聚焦「万物皆可AI」，诚邀AI创业者、开发者、资深使用者，让更多的AI落地被看见，让更多人用上AI、用好AI，被AI加速成长。\\n欢迎来到，「万物皆可AI」时代。\\n过去一年里AIGC产业风起云涌，无数优秀的企业、产品涌现而出，量子位将根据过去一年里AIGC企业、产品的表现与反馈，结合对2024年技术与场景的观察，评选出2025年值得关注的AIGC企业、 2025年值得关注的AIGC产品两类奖项。\\n产品创新力关注产品在功能、体验和应用场景方面的创新性和独特性；\\n本次评选即日起开始报名，截至2025年3月24日，'}, {'name': '通信行业点评报告：字节发布豆包实时语音模型，重视AI产业链', 'url': 'https://stock.stockstar.com/JC2025012100005011.shtml', 'snippet': '（以下内容从开源证券《通信行业点评报告：字节发布豆包实时语音模型，重视AI产业链》研报附件原文摘录）字节发布豆包实时语音大模型发布，AI版图持续扩张2025年1月20日，字节跳动发布豆包实时语音大模型，该模型是一款语音理解和生成一体化的模型，实现了端到端语音对话。相比传统级联模式，该模型在语音表现力、控制力、情绪承接方面表现较强，并具备低时延、对话中可随时打断等特性。在数据收集中，团队筛选并整理大量包含丰富情感的语音数据，涵盖各种场景与情绪状态。在预训练阶段，团队对各模态交织数据进行深入训练，精准捕捉并高效压缩海量语音信息，通过Scaling，最大程度实现语音与文本能力深度融合和能力涌现。在后训练阶段，团队使用了高质量数据与RL算法，进一步提供模型高情商对话能力与安全性，并使大模型同时兼具智商”与情商”。架构支持多模态输入输出，涌现出较强声音控制、声音扮演等能力在架构方面，豆包团队研发端到端框架，深度融合语音与文本模态，该框架面向语音生成和理解进行统一联合建模，实现多模态输入和输出效果，涵盖S2S（语音到语音）、S2T（语音到文本）、T2S（文本到语音）、T2T（文本到文本）等多种模式。模型涌现多种能力：在声音控制方面，模型不仅能依照基础指令输出，还可遵循丰富的复杂指令；在声音扮演方面，目前模型部分方言和口音，主要源自于预训练阶段数据泛化，而非针对性训练。此外，模型支持实时联网功能，能够根据问题，动态获取最新信息，问题回答时效性强。从测评结来看，豆包实时语音大模型在情绪理解和情感表达方面优势明显，整体满意度方面，豆包实时语音大模型评分为4.36，GPT-4o为3.18，豆包模型表现更优。ToB和ToC端大模型齐飞，重视字节跳动等相关产业链投资机会ToB和ToC端大模型齐飞，建议重视国产算力产业链：（1）【国产算力芯片】推荐标的：中兴通讯；受益标的：寒武纪、海光信息等；（2）【字节/阿里/腾讯/百度等AIDC供应商】推荐标的：润泽科技、宝信软件；受益标的：光环新网、世纪互联、大位科技、东方国信、云赛智联、奥飞数据、万国数据、科华数据等；（3）【液冷】推荐标的：英维克；受益标的：申菱环境、同飞股份、网宿科技、科华数据、高澜股份、申菱环境、依米康、飞荣达等；（4）【服务器电源】受益标的：欧陆通、麦格米特等；（5）【柴油发电机】受益标的：科泰电源、潍柴重机等；（6）【变压器】受益标的：金盘科技等；（7）【铜连接】受益标的：'}, {'name': '中文大模型基准测评2024年10月报告-2024年度中文大模型阶段性进展评估', 'url': 'https://www.sohu.com/a/826962523_121825883', 'snippet': '2024年10月的SuperCLUE报告聚焦于中文大模型的最新进展。报告指出，OpenAI的o1-preview在全球大模型竞争中领先，国内模型如Qwen2.5-72B-Instruct和DeepSeek V2.5在全球开源模型中表现突出。\\n国内闭源模型GLM-4-Plus、SenseChat 5.5、AndesGPT-2.0与国际模型差距缩小。报告还强调了端侧小模型的快速发展，特别是在中文场景下的应用潜力。\\n报告目录：\\n报告核心结论摘要\\n国内大模型关键进展及趋势\\nSuperCLUE通用能力测评\\nSuperCLUE专项与行业基准测评\\nSuperCLUE多模态能力测评\\n精准量化通用人工智能（AGI）进展，定义人类迈向AGI的路线图\\n报告部分内容节选如下：\\n返回搜狐，查看更多\\n责任编辑：'}, {'name': '首个可保留情感的音频LLM！Meta重磅开源7B-Spirit LM，一网打尽「音频+文本」多模态任务', 'url': 'https://www.163.com/dy/article/JHJO9R3I0511ABV6.html', 'snippet': '新智元报道\\n编辑：LRS\\n【新智元导读】Meta最近开源了一个7B尺寸的Spirit LM的多模态语言模型，能够理解和生成语音及文本，可以非常自然地在两种模式间转换，不仅能处理基本的语音转文本和文本转语音任务，还能捕捉和再现语音中的情感和风格。\\n在纯文本大模型取得进展的同时，其他模态数据，如语音与文本结合的语言模型（SpeechLMs）也成为了一个热门的研究领域，但现有的模型要么在仅包含语音的数据上进行训练，要么是关注特定任务，如文本转语音（TTS）、自动语音识别（ASR）或翻译，在其他模态数据和任务上的泛化能力十分有限。\\n在大型语言模型（LLM）性能不断提升的情况下，一个常用的方法是先用ASR模型将语音转录成文本，然后用文本模型来生成新的文本，最后再用TTS模型将文本转换成语音，这种流程的一个显著缺陷就是语音表达性不佳，语言模型无法建模并生成富有表现力的语音数据。\\n最近，Meta开源了一个基础多模态语言模型Spirit LM，基于一个70亿参数的预训练文本语言模型，交错使用文本和语音数据进行训练，使模型能够自由地混合文本和语音，在任一模态中生成语言内容。\\n项目主页：https://speechbot.github.io/spiritlm/\\n论文链接：https://arxiv.org/pdf/2402.05755\\n代码链接：https://github.com/facebookresearch/spiritlm\\n开源链接：https://huggingface.co/spirit-lm/Meta-spirit-lm\\n将语音和文本序列拼接成一条token流，并使用一个小型的、自动整理（automatically-curated）的语音-文本平行语料库，采用逐词交错的方法进行训练。\\nSpirit LM有两个版本：基础版（Base）使用语音音素单元（HuBERT），表达版（Expressive）还额外使用音高和风格单元来模拟表达性，以增强模型在生成语音时的表现力，也就是说模型不仅能够理解和生成基本的语音和文本，还能在表达情感和风格方面表现得更加丰富和自然。\\n对于两个版本的模型，文本都使用子词BPE标记进行编码，最终得到的模型既展现了文本模型的语义能力，也具备语音模型的表达能力；模型还能够在少量样本的情况下跨模态学习新任务（例如自动语音识别、文本转语音、语音分类）。\\n不过需要注意的是，和其他预训练模型一样，Sprit LM也可能会生成一些不安全的内容，'}, {'name': '腾讯混元上线文生视频并开源，120秒内成片！还有提示词建议', 'url': 'https://www.sohu.com/a/832903527_115978', 'snippet': '智东西\\n作者  汪越\\n编辑  漠影\\n智东西12月3日报道，今天，腾讯混元大模型正式上线视频生成能力，这是在腾讯文生文、文生图、3D生成之后的最新技术进展。\\n据腾讯混元多模态生成技术负责人凯撒现场介绍，此次更新中，HunYuan-Video模型经历了四项核心改进：\\n1、引入超大规模数据处理系统，提升视频画质；\\n2、采用多模态大语言模型（MLLM），优化文本与图像的对齐；\\n3、使用130亿参数的全注意力机制（DIT）和双模态ScalingLaw，增强时空建模与动态表现；\\n4、采用自研3D VAE架构，提升图像和视频的重建能力。\\n与此同时，腾讯宣布将这款拥有130亿参数规模的视频生成模型开源。目前，该模型已在APP与Web端发布，其标准模式下的视频生成大约需要120秒完成。\\n一、腾讯HunYuan-Video模型技术升级与应用拓展\\n腾讯对HunYuan-Video模型进行了四项技术升级，涵盖了数据处理系统、文本编码、算力优化等多个方面，提升了视频生成的质量与可控性。此外，腾讯还通过微调、应用拓展及开源等措施进一步强化了模型的实际应用能力。\\n1、四项关键技术升级\\n首先，模型采用了一个超大规模的数据处理系统，能够混合处理图像与视频数据。该系统包括文字检测、转景检测、美学打分、动作检测、准确度检测等多个维度的功能，进一步提升视频画质。\\n其次，模型引入了多模态大语言模型（Decoder-only MLLM）作为文本编码器，提升了复杂文本的理解能力，同时支持多语言理解。这一升级使得文本与图像之间的对齐性得到了加强，能够根据用户提供的提示词精确生成符合要求的视频内容。\\n另外，模型架构使用了130亿参数的全注意力机制（DIT）和双模态ScalingLaw，能够在视频生成中有效利用算力和数据资源，增强时空建模能力，并优化视频生成过程中的动态表现。此架构支持原生转场，可实现了多个镜头间的自然切换，并保持主体一致性。\\n最后，HunYuan-Video采用了自研的3D VAE架构，以提升图像和视频重建的能力，特别在小人脸和大幅运动场景下表现更加流畅。\\n2、六大微调领域强化定向能力\\n在预训练之后，腾讯混元大模型目前正在进行微调（SFT）工作，进一步增强其视频生成的定向能力。HunYuan-Video在六个关键方面进行了专项微调，包括画质优化、高动态效果、艺术镜头、手写文本、转场效果以及连续动作的生成，其中一些调整仍在进行中。\\n3、Recaption模型与两种生成模式'}, {'name': '智源研究院与腾讯达成战略合作，推动大模型技术前沿探索和应用落地', 'url': 'https://blog.csdn.net/eagleofstar/article/details/144589920', 'snippet': '2024 年 12 月 18日， 智源研究院与腾讯签署战略合作协议，双方将在大模型研发、人工智能技术前沿探索及开源生态建设等领域展开深入合作。智源研究院院长王仲远、副院长兼总工程师林咏华，腾讯集团高级执行副总裁、云与智慧产业事业群总裁汤道生、腾讯云副总裁、腾讯教育负责人石梅等出席签约仪式。\\n此次战略合作，双方将充分发挥各自优势，共同推动大模型技术与产业场景深度融合，探索多元算力下的大模型训练与推理优化技术解决方案，构建开放创新的软硬件生态体系等。\\n智源研究院院长王仲远在签约仪式上表示，智源研究院是专注于人工智能技术原始创新的新型研发机构，持续引领大模型的技术发展方向，布局多模态世界模型、具身智能、AI for Science 等前沿领域并取得了一系列的国际领先的科研成果与进展；同时，推出了包括模型、数据、算法、评测、系统的大模型全栈开源技术基座以及面向大模型、支持多种异构算力的智算集群软件栈，以赋能产业应用与技术生态的发展。此次，智源研究院携手腾讯，将促进产研协同发展，推动技术转化与产业落地，形成从实验室研究到赋能千行百业的技术生态”。\\n腾讯集团高级执行副总裁，云与智慧产业事业群CEO 汤道生表示，腾讯致力于打造离产业最近的AI，将AI融入到产业场景中。希望腾讯充分发挥在云计算、大数据、人工智能等领域的技术积累，与智源人工智能研究院的研究成果相互融合，共同探索大模型技术的新应用、新场景，探索人工智能技术的无限潜力。'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-22 10:08:33.383\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_answer_zhipu\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mhistory: [{'role': 'system', 'content': '你是一个大型的语言AI助手。当用户提出问题时，请你写出清晰、简洁且准确的答案。我们会给你一组与问题相关的上下文，每个上下文都以类似[[citation:x]]这样的引用编号开始，其中x是一个数字。如果有引用[context]，请在每句话后面使用并引述该上下文。\\n\\n你的答案必须正确、精确，并由专家以公正和专业的语气撰写。请将你的回答限制在1024个token内。如果所提供的上下文信息不足，可以使用自己知识来回答用户问题。\\n\\n请按照[citation:x]格式引用带有参考编号的上下文。如果一句话来自多个上下文，请列出所有适用于此处引述，如[citation:3][citation:5]。除代码、特定名称和引述外，你必须使用与问题相同语言编写你的回答。\\n'}, {'role': 'assistant', 'content': '开源多模态大模型的最新进展主要体现在以下几个方面：\\n\\n1. **MiniMax的开源创新**：MiniMax公司在2025年初发布了开源的MiniMax-01系列模型，包括基础语言大模型MiniMax-Text-01和视觉多模态大模型MiniMax-VL-01。这些模型采用了独有的Lightning Attention机制，借鉴了Linear Attention，显著提升了模型的上下文长度，达到顶尖模型的20-32倍，推理时的上下文窗口可达400万token，对Agent能力的发展具有重要意义[citation:1][citation:2][citation:3]。\\n\\n2. **多模态大语言模型（MLLM）的演进**：MLLM能够处理多模态任务，如视觉问答、图表推理等。其架构包括编码器、连接器和大语言模型，数据训练分为模态对齐和指令微调两阶段。模型演进体现在分辨率提高、输入形式丰富、I/O模态支持增加等方面。团队工作包括缓解幻觉、利用外部反馈提升准确性、构建长视频理解测评基准等[citation:4]。\\n\\n3. **字节跳动的豆包实时语音大模型**：字节跳动发布了豆包实时语音大模型，实现了端到端语音对话，具备低时延、对话中可随时打断等特性。该模型在情绪理解和情感表达方面表现优异，支持多模态输入输出，涵盖S2S、S2T、T2S、T2T等多种模式[citation:6]。\\n\\n4. **Meta的Spirit LM**：Meta开源了7B参数的Spirit LM多模态语言模型，能够理解和生成语音及文本，自然地在两种模式间转换。该模型不仅处理基本的语音转文本和文本转语音任务，还能捕捉和再现语音中的情感和风格[citation:8]。\\n\\n5. **腾讯的HunYuan-Video模型**：腾讯混元大模型上线了视频生成能力，通过引入超大规模数据处理系统、多模态大语言模型、全注意力机制和自研3D VAE架构，提升了视频生成的质量和可控性[citation:9]。\\n\\n这些进展表明，开源多模态大模型在技术架构、应用场景和性能提升方面都取得了显著成果，为AI的进一步发展和应用奠定了坚实基础。'}]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'开源多模态大模型的最新进展主要体现在以下几个方面：\\n\\n1. **MiniMax的开源创新**：MiniMax公司在2025年初发布了开源的MiniMax-01系列模型，包括基础语言大模型MiniMax-Text-01和视觉多模态大模型MiniMax-VL-01。这些模型采用了独有的Lightning Attention机制，借鉴了Linear Attention，显著提升了模型的上下文长度，达到顶尖模型的20-32倍，推理时的上下文窗口可达400万token，对Agent能力的发展具有重要意义[citation:1][citation:2][citation:3]。\\n\\n2. **多模态大语言模型（MLLM）的演进**：MLLM能够处理多模态任务，如视觉问答、图表推理等。其架构包括编码器、连接器和大语言模型，数据训练分为模态对齐和指令微调两阶段。模型演进体现在分辨率提高、输入形式丰富、I/O模态支持增加等方面。团队工作包括缓解幻觉、利用外部反馈提升准确性、构建长视频理解测评基准等[citation:4]。\\n\\n3. **字节跳动的豆包实时语音大模型**：字节跳动发布了豆包实时语音大模型，实现了端到端语音对话，具备低时延、对话中可随时打断等特性。该模型在情绪理解和情感表达方面表现优异，支持多模态输入输出，涵盖S2S、S2T、T2S、T2T等多种模式[citation:6]。\\n\\n4. **Meta的Spirit LM**：Meta开源了7B参数的Spirit LM多模态语言模型，能够理解和生成语音及文本，自然地在两种模式间转换。该模型不仅处理基本的语音转文本和文本转语音任务，还能捕捉和再现语音中的情感和风格[citation:8]。\\n\\n5. **腾讯的HunYuan-Video模型**：腾讯混元大模型上线了视频生成能力，通过引入超大规模数据处理系统、多模态大语言模型、全注意力机制和自研3D VAE架构，提升了视频生成的质量和可控性[citation:9]。\\n\\n这些进展表明，开源多模态大模型在技术架构、应用场景和性能提升方面都取得了显著成果，为AI的进一步发展和应用奠定了坚实基础。'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "step2： 根据网络内容，使用智谱LLM生成总结，并带有[citation:9]引用标记\n",
    "'''\n",
    "from zhipuai import ZhipuAI\n",
    "def generate_answer_zhipu(messages):\n",
    "        client = ZhipuAI(api_key=api_key)  # 请填写您自己的APIKey\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"glm-4-plus\",  # 请填写您要调用的模型名称\n",
    "            messages=messages,\n",
    "            )\n",
    "        response_text = response.choices[0].message.content          \n",
    "        if response_text:\n",
    "            history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "            logger.debug(f\"history: {history}\")\n",
    "        return response_text\n",
    "\n",
    "def generate_answer(request) -> StreamingResponse:\n",
    "        \"\"\"LLM生成服务 - 返回生成的回答\"\"\"\n",
    "        try:\n",
    "            # request = json.loads(request)\n",
    "            # 检测语言\n",
    "            lang = pm.detect_language(request['query'])\n",
    "\n",
    "            # 生成回答\n",
    "            if not history:\n",
    "                content = pm.get_prompt(\"rag_system\", lang)\n",
    "                history.append({\"role\": \"system\", \"content\": content})\n",
    "            context = \"\"\n",
    "            if request['contexts']:\n",
    "                context = \"\\n\\n\".join(\n",
    "                    [f\"[[citation:{i + 1}]] {c.get('snippet', '')}\" for i, c in enumerate(request['contexts'])])\n",
    "            prompt = pm.get_prompt(\n",
    "                \"rag_qa\",\n",
    "                lang,\n",
    "                context=context,\n",
    "                current_date=datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "            )\n",
    "\n",
    "            messages = history + [{\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{request['query']}\"}]\n",
    "            logger.debug(f\"messages: {messages}\")\n",
    "\n",
    "            answer = generate_answer_zhipu(messages)\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in generate: {e}\")\n",
    "            return JSONResponse(\n",
    "                content={\"error\": str(e)},\n",
    "                status_code=503\n",
    "            )   \n",
    "\n",
    "req = {'query':q, 'contexts':contexts}\n",
    "pm = PromptManager()\n",
    "history = []\n",
    "print(req)            \n",
    "\n",
    "generate_answer(req)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-22 10:08:38.993\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_related_questions\u001b[0m:\u001b[36m16\u001b[0m - \u001b[34m\u001b[1mrelated prompt: [context]=```\n",
      "摘要\n",
      "MiniMax，选择用开源「震撼」一下全球同行。\n",
      "作者  Li Yuan\n",
      "编辑  郑玄\n",
      "大模型\n",
      "\n",
      "MiniMax，选择用开源「震撼」一下全球同行。\n",
      "大模型时代已经正式迈入第三年。\n",
      "回首过去的两年，不\n",
      "\n",
      "「2025 年，我们可能会看到第一批 AI Agent 加入劳动力大军，并对公司的生产力产生实质性的\n",
      "```\n",
      "\n",
      "根据原始问题和相关上下文，提出三个相似的问题。不要重复原始问题。每个相关问题应不超过10个token，问题前不要加序号，问题后加问号，`\\n`分隔多个问题。这是原始问题：\n",
      "\n",
      "\n",
      "开源多模态大模型最新进展?\u001b[0m\n",
      "\u001b[32m2025-01-22 10:08:40.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_related_questions\u001b[0m:\u001b[36m36\u001b[0m - \u001b[34m\u001b[1mRelated questions: ['MiniMax开源策略影响?', '大模型时代趋势?', 'AI Agent 劳动力前景?']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "step3： 使用智谱LLM生成相关问题。\n",
    "'''\n",
    "related_history =[]\n",
    "\n",
    "def get_related_questions(query: str, contexts: List[dict]) -> List[str]:\n",
    "        \"\"\"获取相关问题\"\"\"\n",
    "        try:\n",
    "            lang = pm.detect_language(query)\n",
    "            context = \"\"\n",
    "            if contexts:\n",
    "                context = \"\\n\\n\".join([c[\"snippet\"][:50] for c in contexts[:3]])\n",
    "            qa_prompt = pm.get_prompt(\n",
    "                \"related_qa\",\n",
    "                lang,\n",
    "                context=context\n",
    "            )\n",
    "            user_prompt = f\"{qa_prompt}\\n\\n{query}\"\n",
    "            logger.debug(f\"related prompt: {user_prompt}\")\n",
    "            \n",
    "            client = ZhipuAI(api_key=api_key)  # 请填写您自己的APIKey\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"glm-4-plus\",  # 请填写您要调用的模型名称\n",
    "            messages= [{\"role\": \"user\", \"content\": user_prompt}],\n",
    "            max_tokens=512,\n",
    "            )\n",
    "                       \n",
    "            related_history.append({\"role\": \"user\", \"content\": query})            \n",
    "\n",
    "            # 解析回答生成相关问题\n",
    "            answer = response.choices[0].message.content\n",
    "            # \\n分隔，或者？或者?分隔\n",
    "            questions = [q.strip() for q in answer.split('\\n') if q.strip()]\n",
    "            if len(questions) == 1:\n",
    "                questions = [q + '？' for q in answer.split('？') if q.strip()]\n",
    "            if len(questions) == 1:\n",
    "                questions = [q + '?' for q in answer.split('?') if q.strip()]\n",
    "            r = questions[:5]\n",
    "            logger.debug(f\"Related questions: {r}\")\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in generating related questions: {e}\")\n",
    "            return []\n",
    "\n",
    "def get_related( request):\n",
    "        \"\"\"相关问题生成服务\"\"\"\n",
    "        try:\n",
    "            # request = json.loads(request)\n",
    "            # related_history = []\n",
    "\n",
    "            if not related_history:\n",
    "                content = pm.get_prompt(\n",
    "                    \"related_system\",\n",
    "                    pm.detect_language(request['query'])\n",
    "                )\n",
    "                related_history.append({\"role\": \"system\", \"content\": content})\n",
    "\n",
    "            questions = get_related_questions(request['query'], request['contexts'])\n",
    "            return JSONResponse(content=questions)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in related: {e}\")\n",
    "            return JSONResponse(\n",
    "                content={\"error\": str(e)},\n",
    "                status_code=503\n",
    "            )\n",
    "\n",
    "releated_content = get_related(req)                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
